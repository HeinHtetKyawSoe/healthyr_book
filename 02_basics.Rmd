---
output: html_document
editor_options: 
  chunk_output_type: inline
---
# R Basics

```{r, include = FALSE}
library(tidyverse)
```

The aim of this chapter is to familiarise you with how R works. 
We will read in data and start basic manipulations. 
We will be working with a shorter version of the Global Burden of Disease dataset that we met earlier. 


## Getting help

RStudio has a built in Help tab. 
To use the Help tab, click your cursor on something in your code (e.g. `read_csv()`) and press F1. This will show you the definition and some examples. 
However, the Help tab is only useful if you already know what you are looking for but can't remember exactly how it works. 
For finding help on things you have not used before, it is best to Google it. 
R has about 2 million users so someone somewhere has had the same question or problem.


## Objects and functions

The two fundamental concepts to understand about statistical programming are objects and functions. 
As usual, in this book, we prefer introducing new concepts using specific examples first.
And then define things in general terms after examples.

The most common data object you will be working with is a table - so something with rows and columns. 
It should be regular, e.g., the made-up example in Table \@ref(tab:chap2-tab-examp1).
^[Regular does not mean it can't have missing values.
Missing values are denoted `NA` which stands for either `Not available` or `Not applicable`. 
In same contexts, these things can have a different meaning. 
For example, since `var2` is `NA` for all male subjects, it may mean "Not applicable", i.e. something that can only be measured in females.
Whereas in `var3`, `NA` is more likely to mean "Not available" so real missing data, e.g. lost to follow-up.]

```{r chap2-tab-examp1, echo = FALSE}

mydata = tibble(id   = 1:4,
       sex  = c("Male", "Female", "Female", "Male"),
       var1 = c(4, 1, 2, 3),
       var2 = c(NA, 4, 5, NA),
       var3 = c(2, 1, NA, NA))

mydata %>% 
  knitr::kable(booktabs = TRUE, caption = "Example of a table (=tibble once read into R), including missing values denoted NA (Not applicable/Not available).") %>% 
  kableExtra::kable_styling(font_size=8)

```


A table can live anywhere: on paper, in a Spreadsheet, in an SQL database, or it can live in your R Session's Environment. 
And yes, R sessions are as fun as they sound, almost as fun as, e.g., music sessions. 
We usually initiate and interface R using RStudio, but everything we talk about here (objects, functions, sessions, environment) also work when RStudio is not available, but R is. 
This can be the case if you are working on a supercomputer that can only serve the R Console, and not an RStudio IDE (reminder from first chapter: Integrated Development Environment). 
So, regularly shaped data in rows and columns is called table when it lives outside R, but once you read it into R (import it), we call it a tibble.
^[There used to be an older version of tables in R - they are called data frames. 
In most cases, `data frames` and `tibbles` work interchangeably (and both are R objects), but `tibbles` are newer and better. 
Another great alternative to base R's `data frames` are `data tables`. 
In this book, and for most of our day-to-day work these days, we use `tibbles` though.]
When you are in one of your very cool R sessions and read in some data, it goes into this session's Environment. Everything in your Environment needs to have a name as you can have multiple tibbles going on at the same time (`tibble` is not a name, it is the class of an object). To keep our code examples easy to follow, we call our example tibble `mydata`. In real analysis, you should give your tibbles meaningful names, e.g., `patient_data`, `lab_results`, `annual_totals`, etc.

So, the tibble named `mydata` is example of an object that can be in the Environment of your R Session:

```{r}
mydata
```

An example of a function that can be applied on numeric data is `mean()`. 
R functions always have round brackets after their name. 
This is for two reasons. 
First, to easily differentiate them from objects - which don't have round brackets after their name. 
Second, and more important, we can put arguments in these brackets. 
Arguments can also be thought of as input, and in data analysis, the most common input for a function is data: we need to give `mean()` some data to average over. 
It does not make sense (nor will it work) to feed it the whole tibble that has multiple columns, including patient IDs and a categorical variable (`sex`). 
To quickly extract a single column, we use the `$` symbol like this:
\index{symbols@\textbf{symbols}!select column \texttt{\$}}
```{r}
mydata$var1
```

You can ignore the `## [1]` at the beginning of the extracted values - this is something that becomes more useful when printing multiple lines of data as the number in the square brackets keeps count on how many values we are seeing.

We can then use `mydata$var1` as the first argument of `mean()` by putting it inside its brackets:

```{r}
mean(mydata$var1)
```

Which tells us that the mean of `var1` (`r mydata$var1`) is `r mean(mydata$var1)`.
In this example, `mydata$var1` is the first and only argument to `mean()`.
But what happens if we try to calculate the average value of `var2` (`r mydata$var2`)?

```{r}
mean(mydata$var2)
```

We get an `NA` ("Not applicable").
We would expect to see an `NA` if we tried to, for example, calculate the average of `sex`:

```{r, error=TRUE}
mean(mydata$sex)
```

In fact, in this case, R also gives us a pretty clear warning suggesting it can't compute the mean of an argument that is not numeric or logical. 
The sentence actually reads pretty fun, as if R was saying it was not logical to calculate the mean of something that is not numeric. 
But what R is actually saying that it is happy to calculate the mean of two types of variables: numerics or logicals, but what you have passed it is neither.
^[Logical is a data type with two potential values: TRUE or FALSE.
We will come back to data types shortly.]

So `mean(mydata$var2)` does not return an Error, but it also doesn't return the mean of the numeric values included in this column. 
That is because the column includes missing values (`NAs`), and R does not want to average over NAs implicitly.
It is being cautious - what if you didn't know there were missing values for some patients?
If you wanted to compare the means of `var1` and `var2` without any further filtering, you would be comparing samples of different size. 
Which might be fine if the sample sizes are sufficiently representative and the values are missing at random. 
Therefore, if you decide to ignore the NAs and want to calculate the mean anyway, you can do so by adding another argument to `mean()`:

```{r}
mean(mydata$var2, na.rm = TRUE)
```

Adding `na.rm = TRUE` tells R that you are happy for it to calculate the mean of any existing values (but to remove - `rm` - the `NA` values).
This 'removal' excludes the NAs from the calculation, it does not affect the actual tibble (`mydata`) holding the dataset. 
R is case sensitive, so it has look exactly how the function expects it, so `na.rm`, not `NA.rm` etc. 
There is, however, no need to memorize how the arguments of functions are exactly spelled - this is what the Help tab (press `F1` when the cursor is on the name of the function) can remind you of. 
Functions' help pages are built into R, so an internet connection is not required for this.

> Make sure to separate multiple arguments with commas or R will give you an error of `Error: unexpected symbol`.

Finally, some functions do not need any arguments to work.
A good example is the `Sys.time()` which returns the current time and date. 
This is very useful when using R to generate and update reports automatically.
Including this means you can always be clear on when the results were last updated.

```{r}
Sys.time()
```

To summarise, objects and functions work hand in hand. 
Objects are both an input as well as the output of a function (what the function returns). 
The data values input into a function are usually its first argument, further arguments can be used to specify a function's behaviour. 
When we say "the function returns", we are referring to its output (or an Error if it's one of those days).
The returned object can be different to its input object.
In our `mean()` examples above, the input object was a column (`mydata$var1`: `r mydata$var1`), whereas the output was a single value: `r mean(mydata$var1)`.

## Working with Objects

To create a new object into our Environment we use the equals sign:
\index{symbols@\textbf{symbols}!assignment \texttt{=}}

```{r}
a = 103
```

This reads: the variable `a` is assigned value `r a`. You know that the assignment worked when it shows up in the Environment tab.
If we now run `a` just on its own, it gets printed back to us:

```{r}
a
```

Similarly, if we run a function without assignment to a variable, it gets printed but not saved in your Environment:

```{r}
seq(15, 30)
```

`seq()` is a function that creates a sequence of numbers (+1 by default) between the two arguments you pass to it in its brackets. 
We can assign the result of `seq(15, 30)` into a variable, let's call it `example_sequence`:

```{r}
example_sequence = seq(15, 30)
```

Doing this creates `example_sequence` in our Environment, but it does not print it. 

> If you save the results of an R function in a variable, it does not get printed. If you run a function without the assignment (`=`), its results get printed, but not saved in a variable.

You can call your variables (where you assigns new objects or the output of functions in) pretty much anything you want, as long as it starts with a letter. 
It can then include numbers as well, for example, we could have named the new variable `sequence_15_to_30`.
Spaces in variable names are not easy to work with, we tend to use underscores in their place, but you could also use capitalization, e.g. `exampleSequence = seq(15, 30)`.

Finally, R doesn't mind overwriting an existing variable, for example (notice how we then include the variable on a new line to get it printed as well as overwritten):

```{r}
example_sequence = example_sequence/2

example_sequence
```


>Note that many people use `<-` instead of `=`. 
>They mean the same thing in R: both `=` and `<-` save what is on the right into the variable name on the left. 
>There is also a left-to-right operator: `->`.

## Pipe - `%>%` 

\index{symbols@\textbf{symbols}!pipe \texttt{\%>\%}}
The pipe - denoted `%>%` - is probably the oddest looking thing you'll see in this book. But please bear with, it is not as scary as it looks! Furthermore, it is super useful. We use the pipe to send objects into functions.

In the above examples, we calculated the mean of column `var1` from `mydata` by `mean(mydata$var1)`. With the pipe, we can rewrite this as:

```{r}
library(tidyverse)
mydata$var1 %>% mean()
```

Which reads: "Working with `mydata`, we select a single column called `var1` (with the `$`) **and then** calculate the `mean()`." The pipe becomes especially useful once the analysis includes multiple steps applied one after another. A good way to read and think of the pipe is **"and then"**.
This piping business is not standard R functionality and before using it in a script, you need to tell R this is what you will be doing.
The pipe comes from the "magrittr" package (Figure \@ref(fig:chap2-fig-pipe)), but loading the tidyverse will also load the pipe.
So library(tidyverse) initialises everything you need (no need to include library(magrittr) explicitly).


>To insert a pipe `%>%`, use the keyboard shortcut `Ctrl+Shift+M`.


With or without the pipe, the general rule "if the result gets printed it doesn't get saved" still applies. To save the result of the function into a new variable (so it shows up in the Environment), you need to add the name of the new variable with the assignment operator (`=`):

```{r}
mean_result = mydata$var1 %>% mean()
```



```{r chap2-fig-pipe, echo = FALSE, fig.cap="This is not a pipe. René Magritte inspired artwork by Stefan Milton Bache (creator of `%>%` in R). Image source: https://cran.r-project.org/web/packages/magrittr/vignettes/magrittr.html"}
knitr::include_graphics("images/chapter02/magrittr.png")
```

### When pipe sends data to the wrong place: use `, data = .` to direct it

The pipe usually sends data to the beginning of function brackets (as most of the functions we use expect a tibble as the first argument). So `mydata %>% lm(dependent~explanatory)` is equivalent to `lm(mydata, dependent~explanatory)` (`lm()` stands for linear model introduced in detail in Chapter 7: linear regression). 
However, tibble first is not the order the `lm()` function expects. `lm()` wants us to specify the variables first (`dependent~explanatory`), and then wants the tibble these columns are in.
So we have to use the `.` to tell the pipe to send the data to the second argument of `lm()`, not the first, e.g.

```{r, eval = FALSE}
mydata %>% 
  lm(var1~var2, data = .)
```



## Reading data into R

We mentioned before that once a table (e.g. from spreadsheet or database) gets read into R we start calling it a `tibble`. 
The most common format data comes to us in is CSV (comma separated values). 
CSV is basically an uncomplicated spreadsheet with no formatting or objects other than a single table with rows and columns (no worksheets or formulas).
Furthermore, you don't need special software to quickly view a CSV file - a text editor will do, and that includes RStudio.

For example, look at "example_data.csv" in the healthyr project's folder in Figure \@ref(fig:chap2-fig-examplecsv) (this is the Files pane at the bottom-right corner of your RStudio).

```{r chap2-fig-examplecsv, echo = FALSE, fig.cap="View or import a data file."}
knitr::include_graphics("images/chapter02/files_csv_example.png")
```

Clicking on a data file gives us two options: `View File` or `Import Dataset`. 
For very standard CSV files, we don't usually bother with the Import interface and just type in (or copy from a previous script):

```{r, eval = FALSE}
library(tidyverse)
example_data = read_csv("example_data.csv")
```

Without further arguments, `read_csv()` defaults to:

* values are delimited by commas (e.g., `id, var1, var2, ...`)
* numbers use decimal point (e.g., `4.12`), rather than decimal comma (e.g., `4,12`)
* the first line has column names (it is a "header")
* missing values are empty or denoted NA

If your file, however, is different to these, then the `Import Dataset` interface (Figure \@ref(fig:chap2-fig-examplecsv)) is very useful as it will give you the relevant `read_()` syntax with all the extra arguments filled in for you.

```{r chap02-fig-import-tool, echo = FALSE, fig.cap="Import: Some of the special settings your data file might have."}
knitr::include_graphics("images/chapter02/import_options.png")
```


```{r chap02-fig-import-code, echo = FALSE, fig.cap="After using the Import Dataset window, copy-paste the resulting code into your script."}
knitr::include_graphics("images/chapter02/code_preview.png")
```

After selecting the specific options for your import file (there is a friendly preview window too, so you can immediately see whether R understands the format of the your data file), DO NOT BE tempted to press the `Import` button. 
Yes, this will read in your dataset once, but means you have to redo the selections every time you come back to RStudio.
Do copy-paste the code it gives you (e.g., Figure \@ref(fig:chap02-fig-import-code)) into your R script - this way you can use it over and over again. 
Making sure all steps are recorded in scripts makes your workflow reproducible by your future self, colleagues, supervisors, extraterrestrials. 

>The `Import Dataset` button can also help you to read in Excel, SPSS, Stata, or SAS files (instead of `read_csv()`, it will give you `read_excel()`, `read_sav()`, `read_stata()`, or `read_sas()`).

If you've used R before or are trying to make sense of legacy scripts passed on to you by colleagues, you might see `read.csv()` rather than `read_csv()`. In short: `read_csv()` is faster and better, and in all new scripts that's what you should use. But in existing scripts that work and are tested, do not just start replacing `read.csv()` with `read_csv()`. The thing is, `read_csv()` handles categorical variables slightly differently ^[It does not silently convert strings to factors, i.e., it defaults to `stringsAsFactors = FALSE`. For those not familiar with the terminology here - don't worry, we will cover this in just a few sections.]. This means that an R script written using the `read.csv()` might not work as expected any more if just replaced with `read_csv()`.

> Do not start updating and possibly breaking existing R scripts by replacing base R functions with the tidyverse ones we show here. Do use the modern functions in any new code you write. 


### Reading in the Global Burden of Disease example dataset (short version)

In the next few chapters of this book, we will be using the Global Burden of Disease datasets. 
The Global Burden of Disease Study (GBD) is the most comprehensive worldwide observational epidemiological study to date. 
It describes mortality and morbidity from major diseases, injuries and risk factors to health at global, national and regional levels.
^[Global Burden of Disease Collaborative Network.
Global Burden of Disease Study 2017 (GBD 2017) Results.
Seattle, United States: Institute for Health Metrics and Evaluation (IHME), 2018.
Available from http://ghdx.healthdata.org/gbd-results-tool.]

GBD data are publicly available from their website. Table \@ref(tab:chap2-tab-gbd) and Figure \@ref(fig:chap2-fig-gbd) show a very high level version of the project's data with just 3 variables: cause, year, deaths_millions (number of people who die of each cause every year). Later, we will be using a longer dataset with different subgroups and we will show you how to summarise comprehensive datasets yourself.

```{r, message=F}
library(tidyverse)
gbd_short = read_csv("data/global_burden_disease_cause-year.csv")
```

```{r chap2-tab-gbd, echo = FALSE}
gbd_short %>% 
  knitr::kable(booktabs = TRUE,
               linesep = c("", "", "\\addlinespace"),
               align = c("l", "c", "r", "l", "c", "r"),
               caption = "Deaths per year from three broad disease categories (short version of the Global Burden of Disease example dataset).") %>% 
  kableExtra::kable_styling(latex_options = c("hold_position"),
                            font_size = 10)
```

```{r chap2-fig-gbd, echo = FALSE, fig.cap="Causes of death from the Global Burden of Disease dataset (Table \\@ref(tab:chap2-tab-gbd)). Data on (B) is the same as (A) but stacked to show the total (sum) of all causes.", fig.height=6, fig.width=6}
source("1_source_theme.R")
library(patchwork)
p1 = gbd_short %>% 
  ggplot(aes(x = year, y = deaths_millions, fill = cause, colour = cause)) +
  geom_point() +
  geom_line() +
  labs(x = "Year", y = "Deaths per year (millions)") +
  facet_wrap(~cause) +
  theme(legend.position = "none") +
  scale_y_continuous(limits = c(0, 50), expand = c(0, 0)) +
  geom_text(aes(label = (deaths_millions %>% round(0))), colour = "#525252", size = 3, vjust = -0.5)

p2 = gbd_short %>% 
  ggplot(aes(x = year, y = deaths_millions, fill = cause, colour = cause)) +
  geom_col() +
  labs(x = "Year", y = "Deaths per year (millions)") +
  theme(legend.position = "top") +
  scale_y_continuous(limits = c(0, 60), expand = c(0, 0)) +
  # another hardcoded tibble name here:
  scale_x_continuous(breaks = unique(gbd_short$year)) +
  guides(fill=guide_legend(ncol=3))

p1/p2 + plot_annotation(tag_levels = "A")


```

\clearpage

## Operators for filtering data
\index{symbols@\textbf{symbols}!less than \texttt{<}}
\index{symbols@\textbf{symbols}!greater than \texttt{>}}
\index{symbols@\textbf{symbols}!less or equal \texttt{<=}}
\index{symbols@\textbf{symbols}!greater or equal \texttt{>=}}
\index{symbols@\textbf{symbols}!equal \texttt{=}}
\index{symbols@\textbf{symbols}!not \texttt{"!}}
\index{symbols@\textbf{symbols}!AND \texttt{\&}}
\index{symbols@\textbf{symbols}!OR \texttt{"|}}

Operators are symbols that tell R how to handle different pieces of data or objects.
We have already introduced three: `$` (selects a column), `=` (assigns values or results to a variable), and the pipe - `%>%` (sends data into a function).

Other common operators are the ones we use for filtering data - these are called comparison and logical operators.
This may be for creating subgroups, or for excluding outliers or incomplete cases.

The comparison operators that work with numeric data are relatively straightforward: `>, <, >=, <=`.
The first two check whether your values are greater or less than another value, the last two check for "greater than or equal to" and "less than or equal to. These operators are most commonly spotted inside the `filter()` function:

```{r}
gbd_short %>% 
  filter(year < 1995)
```
Here we send the data (`gbd_short`) to the `filter()` and ask it to retain all years that are less than 1995.
The resulting tibble only includes the year 1990.
Now, if we use the `<=` (less than or equal to) operator, both 1990 and 1995 pass the filter:
```{r}
gbd_short %>% 
  filter(year <= 1995)
```

Furthermore, the values either side of the operator could both be variables, e.g., `mydata %>% filter(var2 > var1)`.

To filter for values that are equal to something, we use the `==` operator. For example, the first filtering example was actually equivalent to:

```{r}
gbd_short %>% 
  filter(year == 1995)
```

Accidentally using the single equals (`=` so the assignment operator) is a very common mistake and still occasionally happens to the best of us.
In fact, it happens so often that the error the `filter()` function gives when using the wrong one also reminds us what the correct one was

```{r, error = TRUE}
gbd_short %>% 
  filter(year = 1995)
```

> The answer to 'do you need ==?" is almost always "Yes R, I do, thank you".

But that's just because `filter()` is a clever cookie and used to this very common mistake.
There are other useful functions we use these operators in, but they don't always know to tell us that we've just confused `=` for `==`.
So whenever checking for equality of variables but the result is not what you expect (you'll get an Error, but not necessary with the same wording as above), remember to check your `==` operators first.

R also has two operators for combining multiple comparisons: & and |, which stand for AND and OR, respectively.
For example, we can filter to only keep the earliest and latest years in the dataset:

```{r}
gbd_short %>% 
  filter(year == 1995 | year == 2017)
```

This reads: take the GBD dataset, send it to the filter to keep rows where year is equal to 1995 or year is equal to 2017.
Using specific values like we've done there (1995/2017) is called "hard-coding", which is fine if we know for sure we don't want to apply the same script on an updated dataset. But a cleverer way of achieving the same thing is to use the `min()` and `max()` functions:

```{r}
gbd_short %>% 
  filter(year == max(year) | year == min(year))
```



```{r chap2-tab-filtering-operators, echo = FALSE}
Operators = c("==", "!=" ,"<", ">", "<=", ">=", "&", "|")
Meaning   = c("Equal to", "Not equal to", "Less than", "Greater than", "Less than or equal to", "Greater then or equal to", "AND", "OR")

testdata = tibble(Operators, Meaning)

testdata %>% 
    knitr::kable(booktabs = TRUE,
               linesep = c(rep("", 6), "\\addlinespace"),
               align = c("l", "c", "r"),
               caption = "Filtering operators.") %>% 
  kableExtra::kable_styling(font_size=8)

```


### Worked examples

Filter the dataset to only include the year 2000. Save this in a new variable using the assignment operator.

```{r, echo = TRUE}

mydata_year2000 = gbd_short %>% 
  filter(year == 2000)

```

Let's practice combining multiple selections together.

Reminder: '|' means OR and '&' means AND.

From `gbd_short`, select the lines where year is either 1990 or 2017 and cause is "Communicable diseases":

```{r}

new_data_selection = gbd_short %>% 
  filter((year == 1990 | year == 2013) & cause == "Communicable diseases")

# Or we can get rid of the extra brackets around the years
# by moving cause into a new filter on a new line:

new_data_selection = gbd_short %>% 
  filter(year == 1990 | year == 2013) %>% 
  filter(cause == "Communicable diseases")
```

\index{symbols@\textbf{symbols}!comment \texttt{\#}}
The hash symbol (`#`) is used to add free text comments to R code. R will not try to run these lines, they will be ignored.
Comments are an essential part of any programming code - these are notes for your future self on what and why you did.

## The combine function: c()

The combine function is used to list several values. It is especially useful together with the `%in%` operator which can be used to filter for multiple values. 
Remember how the gbd_short cause column had three different causes in it:

```{r}
gbd_short$cause %>% unique()
```

Say we wanted to filter for communicable and non-communicable diseases.
^[In this example, it would just be  easier to filter(cause `!=` "Injuries") but imagine your column had more than just three different values in it.] We could use the OR operator - `|` like this:

```{r}
gbd_short %>% 
  # also filtering for a single year to keep the result concise
  filter(year == 1990) %>% 
  filter(cause == "Communicable diseases" | cause == "Non-communicable diseases")
```

But that means we have to type in `cause` twice (and even more time if we had more different values we wanted to include).
This where the `%in%` operator together with the `c()` function come in handy:

```{r}
gbd_short %>% 
  filter(year == 1990) %>% 
  filter(cause %in% c("Communicable diseases", "Non-communicable diseases"))
```



## Missing values (NAs) and filters

Filtering for missing values (NAs) needs your special attention and care.
Remember the small example tibble from Table \@ref(tab:chap2-tab-examp1) - it has some NAs in columns `var2` and `var3`:

```{r}
mydata
```


If we now want to filter for rows where `var2` is missing, `filter(var2 == NA)` is not the way to do it, it will not work. 
Since R is a programming language, it can be a bit stubborn with things like these.
When you ask R to do a comparison using `==` (or `<`, `>`, etc.) it expects a value on each side, but NA is not a value, it is the lack thereof.
The way to filter for missing values is using the `is.na()` function:

```{r}
mydata %>% 
  filter(is.na(var2))
```
We send `mydata` to the filter and keep rows where `var2` is `NA`. 
Note the double brackets at the end: that's because the inner one belongs to `is.na()`, and the outer one to `filter()`.
Missing out a closing bracket is also a very common source of (minor, easily fixed) mistakes, and it still happens to the best of us.

If filtering for rows where `var2` is not missing, we do this^[In this simple example, `mydata %>% filter(! is.na(var2))` could be replace by a shorthand: `mydata %>% drop_na(var2)`, but it is important to understand how the ! and `is.na()` work as there will be more complex situations where using these is necessary.]

```{r}
mydata %>% 
  filter(! is.na(var2))
```

In R, the exclamation mark (!) means "not".

Sometimes you want to drop a specific value (e.g. an outlier) from the dataset like this. The small example tibble `mydata` has 4 rows, with the values for `var2` as follows: `r mydata$var2`. We can exclude the row where `var2` is equal to 5 by using the "not equals" (`!=`)^[ `filter(var2 != 5) is equivalent to filter(! var2 == 5)`]:

```{r}
mydata %>% 
  filter(var2 != 5)
```

However, you'll see that by doing this, R drops the rows where `var2` is NA as well, as it can't be sure these missing values were not equal to 5.

If you want to keep the missing values, you need to make use of the OR (`|`) operator and the `is.na()` function:

```{r}
mydata %>% 
  filter(var2 != 5 | is.na(var2))
```

Being caught out by missing values, either in filters or other functions is very common (remember mydata$var2 %>% mean() returns NA unless you add `na.rm = TRUE`). This is also why we insist that you always plot your data first - outliers will reveal themselves and NA values usually become obvious too.

Another thing we do to stay safe around filters and missing values is saving the results and making sure the number of rows still add up:

```{r}
subset1 = mydata %>% 
  filter(var2 == 5)

subset2 = mydata %>% 
  filter(! var2 == 5)

subset1
subset2
```

If the numbers are small, you can now quickly look at RStudio's Environment tab and figure out whether the number of observations (rows) in `subset1` and `subset2` add up to the whole dataset (`mydata`). Or use the `nrow()` function to as R to tell you what the number of rows is in each dataset:

Rows in `mydata`:

```{r}
nrow(mydata)
```

Rows in `subset1`:
```{r}
nrow(subset1)
```

Rows in `subset2`:
```{r}
nrow(subset2)
```

Asking R whether adding these two up equals the original size:

```{r}
nrow(subset1) + nrow(subset2) == nrow(mydata)
```

As expected, this returns FALSE - because we didn't add special handling for missing values.
Let's create a third subset only including rows where `var3` is NA:

Rows in `subset2`:
```{r}
subset3 = mydata %>% 
  filter(is.na(var2))

nrow(subset1) + nrow(subset2) + nrow(subset3) == nrow(mydata)
```



## Variable types and why we care

There are three broad types of data: 

* continuous (numbers), in R: numeric, double, or integer; 
* categorical, in R: character, factor, or logical (TRUE/FALSE); 
* date/time, in R: POSIXct date-time^[Portable Operating System Interface (POSIX) is a set of computing standards. There's nothing more to understand about this other than when R starts shouting "POSIXct this and POSIXlt that" at you, check your date and time variables].


Values within a column all have to be the same type, but a tibble can of course hold columns of different types.
Generally, R is very good at figuring out what the type your data is (in programming, this 'figuring out' is called 'parsing').
For example, when reading in data, it will tell you what it assumed for the columns:

```{r}
library(tidyverse)
typesdata = read_csv("data/typesdata.csv")

typesdata
```

This means that a lot of the time you do not have to worry about those little `<chr>` vs `<dbl>` vs `<S3: POSIXct>` labels, R knows what its doing.
But in cases of irregular or faulty input data, or when doing a lot of calculations and modifications your data, we need to be aware of these different types to be able to find and fix mistakes.

For example, consider a very similar file as above but with a couple of data entry issues:

```{r}
typesdata_faulty = read_csv("data/typesdata_faulty.csv")

typesdata_faulty
```

Notice R now parsed both measurement and date as characters. The first one is a data entry issue: the person taking the measurement couldn't decide which value to note down (maybe the scale was shifting between the two values) so they included both values and text "or" in the cell. A will also get parsed as a categorical variable because of a small typo, e.g., if entered as "3..7" instead of "3.7". And the reason R didn't automatically make sense of the date column is that it can't know whether which one is the month and which one year: __02-Jan-17__ could stand for _02-Jan-2017_ as well as _2002-Jan-17_.

Therefore, while a lot of the time you do not have to worry about variable types and can just get on with your analysis, but it is important to understand what the different types are to be ready to deal with them when issues arise. 

>Furthermore, since health data is generally full of categorical data, it is crucial that you do understand the difference between characters and factors (both are types of categorical variables in R with their pros and cons).

So here we go.

## Numeric variables (continuous)

Number are straightforward to handle and don't usually cause trouble. 
R usually refers to numbers as `numeric` (or `num`), but sometimes it really gets its nerd on and also calls numbers `integer` or `double`.
^[ Integers are numbers without decimal places (e.g., `1, 2, 3`), whereas `double` stands for "Double-precision floating-point" format (e.g., `1.234, 5.67890`).] 
It doesn't usually matter whether R is classifying your continuous data `numeric/num/double/int`, but it is good to be aware of these different terms as you will see them in R messages.

> FRIENDLY WARNING: What's about to follow is a bit dry. Furthermore, it is not essential for complete beginners - you might want to continue reading from __Characters__.
Before you leave, take a mental note that sometimes numbers in R have more decimal places than it seems, and that can cause funny behaviour when using the double equals operator (`==`).

Something to note about numbers is that R doesn't usually print more than 6 decimal places, but that doesn't mean they don't exist.
For example, from the `typedata` tibble, we're taking the `measurement` column and sending it to the `mean()` function. 
R then calculates the mean and tells us what it is with 6 decimal places:


```{r}
typesdata$measurement %>% mean()
```
Let' save that in a new object:

```{r}
measurement_mean = typesdata$measurement %>% mean()
```

But when using the double equals operator to check if this is equivalent to a fixed value (you might do this when comparing to a threshold, or even another mean value), R returns `FALSE`:

```{r}
measurement_mean == 3.333333
```

Now this doesn't seem right, does it - R clearly told us just above that the mean of this variable is 3.333333 (reminder: the actual values in the measurement column are `r typesdata$measurement`). 
The reason the above statement is `FALSE` is because `measurement_mean` is quietly holding more than 6 decimal places.
 
One way to go about this is to round the mean to a reasonable number of decimal places:

```{r}
round(measurement_mean, 3)
```

The second argument of `round()` specifies the number of decimal places you want your number(s) rounded to.
So when using `round()` in the equality statement like this, we get the expected `TRUE`:

```{r}
round(measurement_mean, 3) == 3.333
```


Which is usually fine, especially if you've finished applying calculations on that number. 
But when you indent to use it if further calculations, then rounding should be left to the very end - to minimise rounding errors. 
This is where the `near()` function comes in handy:

```{r}
library(tidyverse)
near(measurement_mean, 3.333, 0.001)
```

The first two arguments for `near()` are the numbers you are comparing, the third argument is the precision you are interested in. So if the numbers are equal within that precision, it returns `TRUE`. This means you get the expected result without having to round the numbers off.

## Character variables (categorical, IDs, free text)

**Characters** (sometimes referred to as *strings* or *character strings*) in R are letters, words, or even whole sentences (an example of this may be free text comments). 
Characters are displayed in-between `""` (or `''`).

A very useful function for quickly investigating categorical variables is the `count()` function:

```{r}
library(tidyverse)
typesdata %>%
  count(group)
```

`count()` can accept multiple variables and will count up the number of observations in each subgroup, e.g., `mydata %>% count(var1, var2)`.

Another helpful option to count is `sort = TRUE`, which will order the result putting the highest count (`n`) to the top.

```{r}
typesdata %>%
  count(group, sort = TRUE)
```

`count()` and its `sort = TRUE` option are also useful for identifying duplicate IDs or misspellings in your data.
With this example `tibble` (`typesdata`) that only has three rows, it is easy to see that the `id` column is a unique identifier whereas the `group` column is a categorical variable.
You can check everything by just eyeballing the `tibble` using the built in Viewer tab (click on the dataset in the Environment tab).

But for larger datasets, you need to know how to check and then clean data programmatically - can't go through 1000s of values checking if they're all the way you expect without unexpected duplicates or typos.
For most variables (categorical or numeric) we recommend always plotting your data before starting analysis.
But to check for duplicates in a unique identifier, use `count()` with `sort = TRUE`:


```{r}
# all ids are unique:
typesdata %>% 
  count(id, sort = TRUE)

# we add in a duplicate row where id = ID3,
# then count again:
typesdata %>% 
  add_row(id = "ID3") %>% 
  count(id, sort = TRUE)

```


## Factor variables (categorical)

**Factors** are fussy characters. 
Factors are fussy because they have something called __levels.__ 
Levels are all the unique values a factor variable could take - e.g. like when we looked at `typesdata$group %>% unique()`.
Using factors rather than just characters can be useful because:


* The values factor levels can take is fixed. 
For example, once you tell R that `typesdata$group` is a factor with two levels: Control and Treatment, combining it with other datasets with different spellings are abbreviations for the same variable would get you a warning.
This can be very helpful and useful, but it can also be a nuisance when you really do want to add in another option for a `factor` variable.
* Levels have an order. 
When running statistical tests on grouped data (e.g., Control vs Treatment, Adult vs Child) and the variable is just a character, not a factor, R will use the alphabetically first as the reference level. 
Converting a character column into a factor column enables us to define and change the order of its levels. 
Level order also affect plots: by default, categorical variables (i.e., think of a barplot) get ordered alphabetically, but if this is not the order we want them in, we have to make it into a factor before we plot it. The plot will then know how the order it better.


So overall, since health data is often categorical and has a reference (comparison) level, then factors are an essential way to work with these data in R.
Nevertheless, the fussiness of factors can sometimes be unhelpful or even frustrating.
It takes experience as an R user to know when is it easiest to keep your variables as characters and when to convert them to factors.
A lot more about factor handling will be covered later in the book.

## Date/time variables

R is very good for working with dates. 
For example, it can calculate the number of days/weeks/months between two dates, or it can be used to find a future date is (i.e., "what's the date exactly 60 days from now?").
It also knows about time zones and is happy to parse dates in pretty much any format - as long as you tell R how your date is formatted (e.g., day before month, month name abbreviated, year in 2 or 4 digits, etc.).
Since R displays dates and times between quotes (""), they look similar to characters. However, it is important to know whether R has understood which of your columns contain date/time information, as which are just normal characters.


```{r, message = FALSE}
library(lubridate) # lubridate makes working with dates easier
current_datetime = Sys.time()
current_datetime

my_datetime = "2020-12-01 12:00"
my_datetime

```

When printed, the two objects - `current_datetime` and `my_datetime` seem to have the a very similar format.
But if we try to calculate the difference between these two dates, we get an error:

```{r, error = TRUE}
my_datetime - current_datetime
```

That's because when we assigned a value to `my_datetime`, R assumed the simpler type for it - so a character.
We can check what the type of an object or variable is using the `class()` function:

```{r}
current_datetime %>% class()
my_datetime %>% class()
```


So we need to tell R that `my_datetime` does indeed include date/time information so we can then use it in calculations:

```{r}
my_datetime_converted = ymd_hm(my_datetime)
my_datetime_converted
```

Calculating the difference will now work:
```{r}
my_datetime_converted - current_datetime
```

Since R knows this is a difference between two date/time objects, it prints the in a nicely readable way.
Furthermore, the result has its own type, it is a "difftime".
```{r}
my_datesdiff = my_datetime_converted - current_datetime
my_datesdiff %>% class()
```

This is useful if we want to apply this time difference on another date, e.g.:

```{r}
ymd_hm("2021-01-02 12:00") + my_datesdiff
```

But if we want to use the number of days in a normal calculation, e.g., what if a measurement increased by 560 arbitrary units during this time period.
We might want to calculate the increase per day like this:


```{r, error = TRUE}
560/my_datesdiff
```

Doesn't work, does it.
We need to convert `my_datesdiff` (which is a difftime value) into a numeric value by using the `as.numeric()` function:

```{r}
560/as.numeric(my_datesdiff)
```

The lubridate package comes with several convenient functions for parsing dates, e.g., `ymd()`, `mdy()`, `ymd_hm()`, etc. - for a full list see lubridate.tidyverse.org.

However, if your date/time variable comes in an extra special format, then use the `parse_date_time()` function where the second argument specifies the format using these helpers:

| Notation | Meaning             | Example |
|----------|---------            |---------|
|`%d`        |day as number        |01-31|
|`%m`        |month as number      |01-12|
|`%B`        |month name           |January-December|
|`%b`        |abbreviated month    |Jan-Dec|
|`%Y`        |4-digit year         |2019|
|`%y`        |2-digit year         |19|
|`%H`        |hours                |12|
|`%M`        |minutes              |01|
|`%A`        |weekday              |Monday-Sunday|
|`%a`        |abbreviated weekday  |Mon-Sun|


For example:

```{r}
parse_date_time("12:34 07/Jan'20", "%H:%M %d/%b'%y")
```

Furthermore, the same date/time helpers can be used to rearrange your date and time for printing:

```{r}
Sys.time()
Sys.time() %>% format("%H:%M on %B-%d (%Y)")
```
You can even add plain text into the `format()` function, R will know to put the right date/time values where the `%` are:

```{r}
Sys.time() %>% format("Happy days, the current time is %H:%M %B-%d (%Y)!")
```



## Creating new columns - `mutate()`


The function for adding new columns (or making changes to existing ones) to a tibble is called `mutate()`.
As a reminder, this is what `typesdata` looked like:

```{r}
typesdata
```

Let's say we decide to divide the column measurement by 2.
A very quick way to see these values would be to pull them out using the `$` operator and then divide by 2:

```{r}
typesdata$measurement
typesdata$measurement/2
```

But this becomes very cumbersome once we want to combine multiple variables from the same tibble in a calculation. So the `mutate()` is the way to go here:

```{r}
typesdata %>% 
  mutate(measurement/2)
```

Notice how the `mutate()` above returns the whole tibble with a new column called `measurement/2`. This is quite nice of `mutate()` already, but it would be best to give columns names that don't include characters other than underscores (`_`) or dots (`.`). So let's assign a more standard name for this new column:

```{r}
typesdata %>% 
  mutate(measurement_half = measurement/2)
```

Better. You can see that R likes the name we gave it a bit better as it's now removed the back-ticks from around it.
Overall, back-ticks can be used to call out non-standard column names, so if you are forced to read in data with, e.g., spaces in column names, then the back-ticks enable calling column names that would otherwise error^[If this happens to you a lot, then check out `library(janitor)` and its function `clean_names()` for automatically tidying non-standard column names.]:

```{r, eval = FALSE}
mydata$`Nasty column name`

# or

mydata %>% 
  select(`Nasty column name`)
```


But as usual, if it gets printed, it doesn't get saved. We have two options - we can either overwrite the `typesdata` tibble (by changing the first line to `typesdata = typesdata %>% `), or we can create a new one (that appears in your Environment):

```{r}
typesdata_modified = typesdata %>% 
  mutate(measurement_half = measurement/2)

typesdata_modified
```


The `mutate()` function can also be used to create a new column with a single constant value, which in return can be used to calculate a difference for each of the existing dates:

```{r}
library(lubridate)
typesdata %>% 
  mutate(reference_date   = ymd_hm("2020-01-01 12:00"),
         dates_difference = reference_date - date) %>% 
  select(date, reference_date, dates_difference)
```

(We are then using the `select()` function to only choose the three relevant columns.)

Finally, the mutate function can be used to create a new column with a summarised value in it, e.g. the mean of another column:

```{r}
typesdata %>% 
  mutate(mean_measurement = mean(measurement))
```

Which in return can be useful for calculating a standardized measurement (i.e. relative to the mean):

```{r}
typesdata %>% 
  mutate(mean_measurement     = mean(measurement)) %>% 
  mutate(measurement_relative = measurement/mean_measurement) %>% 
  select(matches("measurement"))
```

### Worked example/exercise

Round the difference to 0 decimal places using the `round()` function inside a `mutate()`. Then add a clever `matches("date")` inside the `select()` function to choose all matching columns.

Solution:
```{r}
typesdata %>% 
  mutate(reference_date   = ymd_hm("2020-01-01 12:00"),
         dates_difference = reference_date - date) %>% 
  mutate(dates_difference = round(dates_difference)) %>% 
  select(matches("date"))
```

You can shorten this by adding the `round()` function directly around the subtraction, so the third line becomes `dates_difference = round(reference_date - date)) %>%`. But sometimes writing calculations out longer than the absolute minimum can make them easier to understand when you return to an old script months later.
Furthermore, we didn't have to save the `reference_date` as a new column, the calculation could have used the value directly: `mutate(dates_difference = ymd_hm("2020-01-01 12:00") - date) %>%`.
But again, defining it makes it clearer for future self what was done. And it makes `reference_date` available for reuse in more complicated calculations within the tibble.


## Conditional calculations - `if_else()`
And finally, we combine the filtering operators (`==`, `>`, `<`, etc) with the `if_else()` function to create new columns based on a condition.

```{r}
typesdata %>% 
  mutate(above_threshold = if_else(measurement > 3,
                                   "Above three",
                                   "Below three"))
```

We are sending `typesdata` into a `mutate()` function, we are creating a new column called `above_threshold` based on whether `measurement` is greater or less than 3. 
The first argument to `if_else()` is a condition (in this case that measurement is greater than 3), the second argument is the value if the condition is TRUE, and the third argument is the value if the condition is FALSE.
Look at each line in the tibble above and convince yourself that the `threshold` variable worked as expected. 
Then look at the two closing brackets - `))` - at the end of the code and convince yourself they both need to be there.

>`if_else()` and missing values tip: for rows with missing values (NAs), the condition returns neither TRUE or FALSE, it returns NA. 
And that might be fine, but if you want to assign a specific group/label for missing values in the new variable, you can add a fourth argument to `if_else()`, e.g., `if_else(measurement > 3, "Above three", "Below three", "Value missing")`.

## Create labels - `paste()`

The `paste()` function is used to add characters together.
It also works with numbers and dates which will automatically be converted to characters before being pasted together into a single label.
See this example where we use all variables from `typesdata` to create a new column called `plot_label` (we `select()` for printing space):

```{r}
typesdata %>% 
  mutate(plot_label = paste(id,
                            "was last measured at", date,
                            ", and the value was",    measurement)) %>% 
  select(plot_label)
```

The paste is also useful when pieces of information are stored in different columns. For example, consider this made-up tibble:

```{r}
pastedata = tibble(year  = c(2007, 2008, 2009),
                   month = c("Jan", "Feb", "March"),
                   day   = c(1, 2, 3))

pastedata
```

We can use `paste()` to combine these into a single column:

```{r}
pastedata %>% 
  mutate(date = paste(day, month, year, sep = "-"))
```

By default, `paste()` adds a space between the each value, but we can use the `sep = ` argument to specify a different separator. 
Sometimes it is useful to use `paste0()` which does not add anything between the values (no space, no dash, etc.).

We can now tell R that the date column should be parsed as such:

```{r}
library(lubridate)

pastedata %>% 
  mutate(date = paste(day, month, year, sep = "-")) %>% 
  mutate(date = dmy(date))
```


## Joining multiple datasets

It is common that different pieces of information might be kept in different files or tables and that you want to combine them together.
For example, consider you have some demographic information (`id`, `sex`, `age`) in one file:


```{r, message = FALSE}
library(tidyverse)
patientdata = read_csv("data/patient_data.csv")
patientdata
```

And another one with some lab results (`id`, `measurement`):

```{r, message = FALSE}
labsdata = read_csv("data/labs_data.csv")
labsdata
```

Notice how these datasets are not only different size (`r nrow(patientdata)` rows in `patientdata`, `r nrow(labsdata)` rows in `labsdata`), but include information on different patients: `patiendata` has ids `r patientdata$id`, `labsdata` has ids `r labsdata$id`.

A comprehensive way to join these is to use `full_join()` retaining all information from both tibbles (and matching up rows by shared columns, in this case `id`):

```{r}
full_join(patientdata, labsdata)
```

However, if we are only interested in matching information, we use the inner join:

```{r}
inner_join(patientdata, labsdata)
```

And finally, if we want to retain all information from one tibble, we use either the `left_join()` or the `right_join()`:


```{r}
left_join(patientdata, labsdata)
right_join(patientdata, labsdata)
```

### Further notes about the joins

* The joins functions (`full_join()`, `inner_join()`, `left_join()`, `right_join()`) will automatically look for matching column names. You can use the `by = ` argument to specify by hand. This is especially useful if the columns are named differently in the datasets, e.g. `left_join(data1, data2, by = c("id" = "patient_id"))`.

* The rows do not have to be ordered, the joins match on values within the rows, not the order of the rows within the tibble.

* Joins are used to combine different variables (columns) into a single tibble. __If you are getting more data of the same variables, use `bind_rows()` instead__:

```{r, message = FALSE}
patientdata_new = read_csv("data/patient_data_updated.csv")
patientdata_new
bind_rows(patientdata, patientdata_new)
```

Finally, it is important to understand how joins behave if there are multiple matches within the tibbles. For example, if patient id 4 had a second measurement as well:

```{r}
labsdata_updated = labsdata %>% 
  add_row(id = 5, measurement = 2.49)
labsdata_updated
```

When we now do a `left_join()` with our main `tibble` - `patientdata`:


```{r}
left_join(patientdata, labsdata_updated)
```

We get 7 rows, instead of 6 - as patient id 5 now appears twice with the two different measurements. So it is important to keep either know your datasets very well or keep an eye on the number of rows to make sure any increases/decreases in the tibble sizes are as you expect them to be.



