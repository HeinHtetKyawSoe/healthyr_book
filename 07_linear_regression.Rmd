# Linear regression
\index{linear regression@\textbf{linear regression}}

> Smoking is one of the leading causes of statistics.  
> Fletcher Knebel

## Regression

Regression is a method by which we can determine the existence and strength of the relationship between two or more variables. 
This can be thought of as drawing lines, ideally straight lines, through data points. 

Linear regression is our method of choice for examining continuous outcome variables. 
Broadly, there are often two separate goals in regression:

* Prediction: fitting a predictive model to an observed dataset. Using that model to make predictions about an outcome from a new set of explanatory variables;
* Explanation: fit a model to explain the inter-relationships between a set of variables.

Figure \@ref(fig:chap07-fig-regression) unifies the terms we will use throughout. 
A clear scientific question should define our `explanatory variable of interest` $(x)$, which sometimes gets called an exposure, predictor, or independent variable. 
Our outcome of interest will be referred to as the `dependent` variable or outcome $(y)$; it is sometimes referred to as the response. 
In simple linear regression, there is a single explanatory variable and a dependent variable, and we will sometimes refer to this as *univariable linear regression*. 
When there is more than one explanatory variable, we will call this *multivariable regression*. 
Avoid the term *multivariate regression*, which suggests more than one dependent variable. 
We don't use this method and we suggest you don't either!

Note that the dependent variable is always continuous, it cannot be a categorical variable. 
The explanatory variables can be either continuous or categorical. 

### The Question (1)

We will illustrate our examples of linear regression using a classical question which is important to many of us!
This is the relationship between coffee consumption and blood pressure (and therefore cardiovascular events, such as myocardial infarction and stroke).
There has been a lot of backwards and forwards over decades about whether coffee is harmful, has no effect, or is in fact beneficial. 
Figure \@ref(fig:chap07-fig-regression) shows a linear regression example. 
Each point is a person.
The explanatory variable is average number of cups of coffee per day $(x)$ and systolic blood pressure as the dependent variable $(y)$. 
This next bit is important!
These data are made up, fake, randomly generated, fabricated, not real. 
So please do not alter your coffee habit on the basis of these plots!

```{r chap07-fig-regression, echo = FALSE, fig.cap="The anatomy of a regression plot."}
knitr::include_graphics("images/chapter07/1_regression_terms.pdf")
```

### Fitting a regression line
\index{linear regression@\textbf{linear regression}!fitted line}

Simple linear regression uses the *ordinary least squares* method for fitting. 
The details are beyond the scope here, but if you want to get out the linear algebra/matrix maths you did in high school, an enjoyable afternoon can be spent proving to yourself how it actually works. 

Figure \@ref(fig:chap07-fig-residuals) aims to make this easy to understand. 
The maths defines a line which best fits the data provided. 
For the line to fit best, the distances between it and the observed data should be as small as possible. 
The distance from each observed point to the line is called a *residual* - one of those statistical terms that bring on the sweats. 
It just refers to the "residual error" left over after the line is fitted. 

You can use the [simple regression shiny app](https://argoshare.is.ed.ac.uk/simple_regression) to explore the concept. 
We want the residuals to be as small as possible. 
We can square each residual (to get rid of minuses and make the algebra more convenient) and add them up. 
If this number is as small as possible, the line is fitting as best it can. 
Or in more formal language, we want to minimise the sum of squared residuals. 

```{r chap07-fig-residuals, echo = FALSE, fig.cap="How a regression line is fitted."}
knitr::include_graphics("images/chapter07/2_residuals.pdf")
```

### When the line fits well

Linear regression modelling has four main assumptions:

1. Linear relationship between between predictors and outcome;
2. Independence of residuals;
3. Normal distribution of residuals;
4. Equal variance of residuals. 
\index{linear regression@\textbf{linear regression}!assumptions}

You can use the [simple regression diagnostics shiny app](https://argoshare.is.ed.ac.uk/simple_regression_diagnostics) to get a handle on these. 

Figure \@ref(fig:chap07-fig-diags) shows diagnostic plots from the app, which we will run ourselves below. 

*Linear relationship*

A simple scatter plot should show a linear relationship between the explanatory and the dependent variable, as in figure \@ref(fig:chap07-fig-diags)A. 
If the data describe a non-linear pattern (figure \@ref(fig:chap07-fig-diags)B), then a straight line is not going to fit it well. 
In this situation, an alternative model should be considered, such as including a quadratic ($x^2$) term. 

*Independence of residuals*

The observations and therefore the residuals should be independent.
This is more commonly a problem in time series data, where observations may be correlated across time with each other (autocorrelation). 

*Normal distribution of residuals*

The observations should be normally distributed around the fitted line. 
This means that the residuals should show a normal distribution with a mean of zero (figure \@ref(fig:chap07-fig-diags)A).
If the observations are not equally distributed around the line, the histogram of residuals will be skewed and a normal Q-Q plot will show residuals diverging from the 45 degree line (figure \@ref(fig:chap07-fig-diags)B) (see section \@ref(chap06-h3-qq-plot)).

*Equal variance of residuals*

The distribution (spread) of the observations around the fitted line should be the same on the left side as on the right side. 
Look at the fan-shaped data on the [simple regression diagnostics shiny app](https://argoshare.is.ed.ac.uk/simple_regression_diagnostics). 
This should be obvious on the residuals vs. fitted values plot, as well as the histogram and normal Q-Q plot. 

This is really all about making sure that the line you draw through your data points is valid.
It is about ensuring that the regression line is appropriate across the range of the explanatory variable and dependent variable.
It is about understanding the underlying data, rather than relying on a fancy statistical test that gives you a *p*-value. 

```{r chap07-fig-diags, echo = FALSE, fig.cap="Regression diagnostics. Does this also appear in the contents. What about this?"}
knitr::include_graphics("images/chapter07/3_diags.pdf")
```

### The fitted line and the linear equation

We promised to keep the equations to a minimum, but this one is so important it needs to be included. But it is easy to understand, so fear not. 

Figure \@ref(fig:chap07-fig-equation) links the fitted line, the linear equation, and the output from R. Some of this will likely be already familiar to you. 

Figure \@ref(fig:chap07-fig-equation)A shows a scatter plot with fitted lines from a multivariable linear regression model. 
The plot is taken from the [multivariable regression shiny app](https://argoshare.is.ed.ac.uk/multi_regression/). 
Remember, these data are simulated and are not real. 
This app will really help you understand different regression models, more on this below. 
The app allows us to specify "the truth" with the sliders on the left hand side. 
For instance, we can set the $intercept=1$, meaning that when all $x=0$, the value of the dependent variable, $y=1$.  

Our model has a continuous explanatory variable of interest (average coffee consumption) and a further categorical variable (smoking). 
In the example the truth is set as $intercept=1$, $\beta_1=1$ (true effect of coffee on blood pressure, gradient/slope of line), and $\beta_2=2$ (true effect of smoking on blood pressure).
The points on the plot are simulated following the addition of random noise. 

Figure \@ref(fig:chap07-fig-equation)B shows the default output in R for this linear regression model. 
Look carefully and make sure you are clear how the fitted lines, the linear equation, and the R output fit together. 
In this example, the random sample from our true population specified above shows $intercept=0.67$, $\beta_1=1.00$ (coffee), and $\beta_2=2.48$ (smoking). 
A *p*-value is provided ($Pr(> \left| t \right|)$), which is the result of a null hypothesis significance test for the gradient of the line being equal to zero. 

```{r chap07-fig-equation, echo = FALSE, fig.cap="Linking the fitted line, regression equation and R output."}
knitr::include_graphics("images/chapter07/4_equation.pdf")
```

### Effect modification
\index{linear regression@\textbf{linear regression}!effect modification}
\index{effect modification}

Effect modification occurs when the size of the effect of the explanatory variable of interest (exposure) on the outcome (dependent variable) differs depending on the level of a third variable. 
Said another way, this is a situation in which an explanatory variable differentially (positively or negatively) modifies the observed effect of another explanatory variable on the outcome.

Again, this is best thought about using the concrete example provided in the [multivariable regression shiny app](https://argoshare.is.ed.ac.uk/multi_regression/).

Figure \@ref(fig:chap07-fig-dags) shows three potential causal pathways. 

In the first, smoking is not associated with the outcome (blood pressure) or our explanatory variable of interest (coffee consumption). 

In the second, smoking is associated with elevated blood pressure, but not with coffee consumption. 
This is an example of effect modification. 

In the third, smoking is associated with elevated blood pressure and with coffee consumption. 
This is an example of confounding. 

```{r chap07-fig-dags, echo = FALSE, fig.cap="Causal pathways, effect modification and confounding."}
knitr::include_graphics("images/chapter07/5_dags.pdf")
```

*Additive vs. multiplicative effect modification (interaction)*
\index{linear regression@\textbf{linear regression}!interactions}
\index{interaction terms}

Depending on the field you work, will depend on which set of terms you use. 
Effect modification can be additive or multiplicative. 
We refer to multiplicative effect modification as simply including a statistical interaction. 

Figure \@ref(fig:chap07-fig-types) should make it clear exactly how these work. 
The data have been set-up to include an interaction term. 
What does this mean? 

* $intercept=1$: the blood pressure ($\hat{y}$) for non-smokers who drink no coffee (all $x=0$);
* $\beta_1=1$ (`coffee`): the additional blood pressure for each cup of coffee drunk by non-smokers (slope of the line when $x_2=0$);
* $\beta_2=1$ (`smoking`): the difference in blood pressure between non-smokers and smokers who drink no coffee ($x_1=0$);
* $\beta_3=1$ (`coffee:smoking` interaction): the blood pressure ($\hat{y}$) in addition to $\beta_1$ and $\beta_2$, for each cup of coffee drunk by smokers ($x_2=1)$.

You may have to read that a couple of times in combination with looking at Figure \@ref(fig:chap07-fig-types).

With the additive model, the fitted lines for non-smoking vs smoking are constrained to be parallel. 
Look at the equation in Figure \@ref(fig:chap07-fig-types)B and convince yourself that the lines can never be anything other than parallel. 

A statistical interaction (or multiplicative effect modification) is a situation where the effect of an explanatory variable on the outcome is modified in non-additive manner. 
In other words using our example, the fitted lines are no longer constrained to be parallel.

If we had not checked for an interaction effect, we would have inadequately described the true relationship between these three variables. 

What does this mean back in reality? 
Well it may be biologically plausible for the effect of smoking on blood pressure to increase multiplicatively due to a a chemical interaction between cigarette smoke and caffeine, for example.

Note, we are just trying to find a model which best describes the underlying data. 
All models are approximations of reality. 

```{r chap07-fig-types, echo = FALSE, fig.cap="Multivariable linear regression with additive and multiplicative effect modification."}
knitr::include_graphics("images/chapter07/6_types.pdf")
```

### R-squared and model fit
\index{linear regression@\textbf{linear regression}!r-squared}
\index{r-squared

Figure \@ref(fig:chap07-fig-types) includes a further metric from the R output: `Adjusted R-squared`. 

R-squared is another measure of how close the data are to the fitted line. 
It is also known as the coefficient of determination and represents proportion of the dependent variable which is explained by the explanatory variable(s). 
So 0.0 indicates that none of the variability in the dependent is explained by the explanatory (no relationship between data points and fitted line) and 1.0 indicates that the model explains all of the variability in the dependent (fitted line follows data points exactly).

R provides the `R-squared` and the `Adjusted R-squared`. 
The adjusted R-squared includes a penalty the more explanatory variables are included in the model. 
So if the model includes variables which do not contribute to the description of the dependent variable, the adjusted R-squared will be lower. 

Looking again at Figure \@ref(fig:chap07-fig-types), in A, a simple model of coffee alone does not describe the data well (adjusted R-squared 0.38). 
Add smoking to the model improves the fit as can be seen by the fitted lines (0.87). But a true interaction exists in the actual data. 
By including this interaction in the model, the fit is very good indeed (0.93).

### Confounding
\index{linear regression@\textbf{linear regression}!confounding}
\index{confounding}

The last important concept to mention here is confounding. 
Confounding is a situation in which the association between an explanatory variable (exposure) and outcome (dependent variable) is distorted by the presence of another explanatory variable.

In our example, confounding exists if there is an association between smoking and blood pressure AND smoking and coffee consumption (Figure \@ref(fig:chap07-fig-dags)C). 
This exists simply if smokers drink more coffee than non-smokers. 

Figure \@ref(fig:chap07-fig-confounding) shows this really clearly. 
The underlying data have been altered so that those who drink more than two cups of coffee per day also smoke and those who drink fewer than two cups per day do not smoke. 
A true effect of smoking on blood pressure is entered, but NO effect of coffee on blood pressure. 

If we simply fit blood pressure by coffee consumption (Figure \@ref(fig:chap07-fig-confounding)A), then we may mistakenly conclude a relationship between coffee consumption and blood pressure. 
But this does not exist, because the ground truth we have set is that no relationship exists between coffee and blood pressure. 
We are simply seeing the effect of smoking on blood pressure, which is confounding the effect of coffee on blood pressure. 

If we include the confounder in the model by adding smoking, the true relationship becomes apparent. 
Two parallel flat lines indicating no effect of coffee on blood pressure, but a relationship between smoking and blood pressure. 
This procedure is often referred to as controlling for or adjusting for confounders. 

```{r chap07-fig-confounding, echo = FALSE, fig.cap="Multivariable linear regression with confounding of coffee drinking by smoking."}
knitr::include_graphics("images/chapter07/7_confounding.pdf")
```

### Summary
We have intentionally spent some time going through the principles and applications of linear regression because it is so important. 
A firm grasp of these concepts lead to an easy understanding of other regression procedures, such as logistic regression and Cox Proportional Hazards regression. 

We will now perform all this ourselves in R using a gapminder dataset which you are familiar with from preceding chapters. 

## Fitting simple models

### The Question (2)
We are interested in modelling the change in life expectancy for different countries over the past 60 years.  

### Get the data
```{r, message=F}
library(tidyverse)
library(gapminder) # dataset
library(lubridate) # handles dates
library(finalfit)
library(broom)

theme_set(theme_bw())
mydata = gapminder
```

### Check the data
Always check a new dataset, as described in section \@ref{chap06-h2-check}.

```{r eval=FALSE}
glimpse(mydata) # each variable as line, variable type, first values
missing_glimpse(mydata) # missing data for each variable
ff_glimpse(mydata) # summary statistics for each variable
```

### Plot the data

Let's plot the life expectancies in European countries over the past 60 years, focussing on the UK and Turkey. We can add in simple best fit lines using `ggplot` directly.

```{r, fig.height=4, fig.width=7, fig.cap="Scatterplot with fitted line plot: Life expectancy by year in European countries"}
p1 = mydata %>%                        # save as object p1
  filter(continent == "Europe") %>%    # Europe only
  ggplot(aes(x = year, y = lifeExp)) + # lifeExp~year  
  geom_point() +                       # plot points
  facet_wrap(~ country) +              # facet by country
  scale_x_continuous(
    breaks = c(1960, 2000))      # adjust x-axis 

p2 = p1 + geom_smooth(method = "lm")   # add regression line

library(patchwork)
p1 + p2
```

### Simple linear regression
\index{functions@\textbf{functions}!lm}

As you can see, `ggplot()` is very happy to run and plot linear regression models for us.
While this is sometimes convenient, we usually want to build, run, and explore these models ourselves. 
We can then investigate the intercepts and the slope coefficients (linear increase per year):

First let's plot two countries to compare, Turkey and United Kingdom

```{r fig.height=3, fig.width=4.5, fig.cap="Scatterplot: Life expectancy by year Turkey and Europe."}
mydata %>% 
  filter(country %in% c("Turkey", "United Kingdom")) %>% 
  ggplot(aes(x = year, y = lifeExp, colour = country)) + 
  geom_point()
```

The two non-parallel lines may make you think of what has been discussed above. 

First, let's model the two countries separately. 

```{r}
# United Kingdom
fit_uk = mydata %>%
  filter(country == "United Kingdom") %>% 
  lm(lifeExp~year, data = .)

fit_uk %>% 
  summary()
```


```{r}
# Turkey
fit_turkey = mydata %>%
  filter(country == "Turkey") %>% 
  lm(lifeExp~year, data = .)

fit_turkey %>% 
  summary()
```


*Accessing the coefficients of linear regression*
\index{linear regression@\textbf{linear regression}!coefficients}

A simple linear regression model will return two coefficients - the intercept and the slope (the second returned value). 
Compare this to the `summary()` output above.

```{r}
fit_uk$coefficients
```

```{r}
fit_turkey$coefficients
```

In this example, the intercept is telling us that life expectancy at year 0 in the United Kingdom (some 2000 years ago) was -294 years. While this is mathematically correct (based on the data we have), it obviously makes no sense in practice. It is important at all stages of data analysis, to keep "sense checking" your results.

To make the intercepts meaningful, we will add in a new column called `year_from1952` and re-run `fit_uk` and `fit_turkey` using `year_from1952` instead of `year`.

```{r}
mydata = mydata %>% 
  mutate(year_from1952 = year - 1952)

fit_uk = mydata %>%
  filter(country == "United Kingdom") %>% 
  lm(lifeExp ~ year_from1952, data = .)

fit_turkey = mydata %>%
  filter(country == "Turkey") %>% 
  lm(lifeExp ~ year_from1952, data = .)
```


```{r}
fit_uk$coefficients
```


```{r}
fit_turkey$coefficients
```

Now, the updated results tell us that in year 1952, the life expectancy in the United Kingdom was 68 years. 
Note that the slope (0.18) does not change. There was nothing wrong with the original model and the results were correct, the intercept was just not very useful.

*Accessing all model information `tidy()` and `glance()`*
\index{functions@\textbf{functions}!tidy}
\index{functions@\textbf{functions}!glance}

In the fit_uk and fit_turkey examples above, we were using `fit_uk %>% summary()` to get R to print out a summary of the model. This summary is not, however, in a rectangular shape so we can't easily access the values or put them in a table/use as information on plot labels.

We use the `tidy()` function from `library(broom)` to get the explanatory variable specific values in a nice tibble:

```{r}
fit_uk %>% tidy()
```

In the `tidy()` output, the column `estimate` includes both the intercepts and slopes.

And we use the `glance()` function to get overall model statistics (mostly the r.squared).
```{r}
fit_uk %>% glance()
```

### Multivariable linear regression
\index{linear regression@\textbf{linear regression}!multivariable}

Multivariable linear regression includes more than one explanatory variable. There are a few ways to include more variables, depending on whether they should share the intercept and how they interact:

Simple linear regression (exactly one predictor variable):

`myfit = lm(lifeExp ~ year, data = mydata)`

Multivariable linear regression (additive):

`myfit = lm(lifeExp ~ year + country, data = mydata)`

Multivariable linear regression (interaction):

`myfit = lm(lifeExp ~ year * country, data = mydata)`

This equivalent to:
`myfit = lm(lifeExp ~ year + country + year:country, data = mydata)`

These examples of multivariable regression include two variables: `year` and `country`, but we could include more by adding them with `+`.

In this particular setting, it will become obvious which model is appropriate. So we have complete control over the model being fitted, we will use the `predict()` function directly to obtain our fitted line, rather than leaving it up to `ggplot`.  

*Model 1: year only*

```{r fig.height=3, fig.width=4.5, fig.cap="Scatter and line plot. Life expectancy in Turkey and the UK - univariable fit."}
mydata_UK_T = mydata %>% 
  filter(country %in% c("Turkey", "United Kingdom"))

fit_both1 = mydata_UK_T %>% 
  lm(lifeExp ~ year_from1952, data = .)
fit_both1

pred_both1 = predict(fit_both1)

mydata_UK_T %>% 
  bind_cols(pred_both = pred_both1) %>% 
  ggplot() + 
  geom_point(aes(x = year, y = lifeExp, colour = country)) +
  geom_line(aes(x = year, y = pred_both))
```

By fitting year only, the model ignores country. This gives us a fitted line which is the average of life expectancy in the UK and Turkey. This may be desirable, depending on the question. But here we want to best describe the data. 

*Model 2: year + country*
```{r fig.height=3, fig.width=4.5, fig.cap="Scatter and line plot. Life expectancy in Turkey and the UK - multivariable additive fit."}
fit_both2 = mydata_UK_T %>% 
  lm(lifeExp ~ year_from1952 + country, data = .)
fit_both2

pred_both2 = predict(fit_both2)

mydata_UK_T %>% 
  bind_cols(pred_both = pred_both2) %>% 
  ggplot() + 
  geom_point(aes(x = year, y = lifeExp, colour = country)) +
  geom_line(aes(x = year, y = pred_both, colour = country))
```

This is better, by including country in the model, we now have fitted lines better represent the data. However, the lines are constrained to be parallel. This is discussed in detail above. We need to include an interaction term to allow the effect of year on life expectancy to vary by country in a non-additive manner. 

*Model 3: year &ast; country*

```{r fig.height=3, fig.width=4.5, fig.cap="Scatter and line plot. Life expectancy in Turkey and the UK - multivariable multiplicative fit."}
fit_both3 = mydata_UK_T %>% 
  lm(lifeExp ~ year_from1952 * country, data = .)
fit_both3

pred_both3 = predict(fit_both3)

mydata_UK_T %>% 
  bind_cols(pred_both = pred_both3) %>% 
  ggplot() + 
  geom_point(aes(x = year, y = lifeExp, colour = country)) +
  geom_line(aes(x = year, y = pred_both, colour = country))
```

This fits the data much better than the previous two models. You can check the R-squared using `summary(fit_both1)`.

*Pro tip*

```{r}
library(purrr)
list(fit_both1, fit_both2, fit_both3) %>% 
  map_df(glance, .id = "fit")

```
\index{functions@\textbf{functions}!purrr::map_df}

### Check assumptions

The assumptions of linear regression can be checked with diagnostic plots, either by passing the fitted object (`lm()` output) to base R `plot()`, or by using the more convenient function below. 

```{r fig.height=5, fig.width=5}
library(ggfortify)
autoplot(fit_both3)
```

There is no clear problem with the residuals, as we would expect from the scatterplot with fitted lines. 

## Fitting more complex models

### The Question (3)

Finally in this chapter, we are going to fit a more complex linear regression model. Here, we will discuss variable selection and introduce the Akaike Information Criterion (AIC). 

We will introduce a new dataset: The Western Collaborative Group Study. This classic includes data from 3154 healthy young men aged 39-59 from the San Francisco area who were assessed for their personality type. It aimed to capture the occurrence of coronary heart disease over the following 8.5 years. 

We will use it however to explore the relationship between systolic blood pressure (`sbp`) and personality type (`personality_2L`), accounting for potential confounders such as weight (`weight`). 
Now this is just for fun - don't write in!
The study was designed to look at cardiovascular events as the outcome, not blood pressure. 
But it is a convenient to use blood pressure as a continuous outcome from this dataset, even if that was not the intention of the study.

Personality type is A: aggressive and B: passive. 

### Model fitting principles
\index{linear regression@\textbf{linear regression}!model fitting principles}

We suggest building statistical models on the basis of the following six pragmatic principles: 

1. As few explanatory variables should be used as possible (parsimony);
2. Explanatory variables associated with the outcome variable in previous studies should be accounted for; 
3. Demographic variables should be included in model exploration; 
4. Population stratification should be incorporated if available; 
5. Interactions should be checked and included if influential; 
6. Final model selection should be performed using a "criterion-based approach"
  + minimise the Akaike information criterion (AIC)
  + maximise the adjusted R-squared value.

  <!-- + maximise the c-statistic (area under the receiver operator curve). -->

This is not the law, but it probably should be. 
These principles are sensible as we will discuss through the rest of this book.
We strongly suggest you do not use automated methods of variable selection. 
These are often "forward selection" or "backward elimination" methods for including or excluding particular variables on the basis of a statistical property.

In certain settings, these approaches may be found to work. 
However, they create an artificial distance between you and the problem you are working on. 
They give you a false sense of certainty that the model you have created is in some sense valid. 
And quite frequently, they will get it wrong. 

Alternatively, you can follow the six principles above.

A variable may have previously been shown to strongly predict an outcome (think smoking and risk of cancer). 
This should give you good reason to consider it in your model. 
But perhaps you think that previous studies were incorrect, or that the variable is confounded by another. 
All this is fair, but it will be expected that this new knowledge is clearly demonstrated by you, so do not omit these variables before you start.

There are particular variables that are so commonly associated with particular outcomes in healthcare that they should almost always be included at the start. 
Age, sex, social class, and co-morbidity for instance are commonly associated with  survival, before you start looking at your explanatory variable of interest or checking that your randomised controlled trial is indeed balanced. 

Patients are often clustered by a particular grouping variable, such as treating hospital. 
There will be commonalities between these patients that are likely not fully explained by your observed variables. 
To estimate the coefficients of your variables of interest most accurately, clustering should be accounted for in the analysis. 

As demonstrated above, the purpose of the model is to provide a best fit approximation of the underlying data. 
Effect modification and interactions commonly exist in heath datasets, and should be incorporated if present. 

Finally, we want to assess how well our models fit the data with 'model checking'. 
The effect of adding or removing one variable to the coefficients of the other variables in the model is very important, and will be discussed later. 
Measures of goodness-of-fit such as the `AIC`, can also be of great use when deciding which model choice is most valid. 

### AIC
\index{linear regression@\textbf{linear regression}!AIC}
\index{AIC}

The Akaike Information Criterion (AIC) is an alternative goodness-of-fit measure. 
In that sense, it is similar to the R-squared, but it has a different statistical basis. 
It is useful because it can be used to help guide the best fit in generalised linear models such as logistic regression (see chapter \@ref(chap09-h1). 
It is based on the likelihood but is also penalised for the number of variables present in the model. We aim to have as small an AIC as possible. 
The value of the number itself has no inherent meaning, but it is used to compare different models. 

### Get the data

```{r}
mydata = finalfit::wcgs #F1 here for details
```

### Check the data

As always, when you receive a new dataset, carefully check that it does not contain errors. 

```{r message=FALSE, include=FALSE}
library(dplyr)
library(finalfit)
glimpse(mydata) # each variable as line, variable type, first values
missing_glimpse(mydata) # missing data for each variable
sum_wcgs = ff_glimpse(mydata) # summary statistics for each variable
```

```{r echo=FALSE, message=FALSE}
library(knitr)
library(kableExtra)
sum_wcgs[[1]] %>%
  select(-c(5, 8, 9, 11, 12)) %>% 
  kable(row.names = FALSE, align = c("l", "l", "l", "r", "r", "r", "r", "r", "r", "r"), 
        booktabs = TRUE, caption = "WCGS data, ff\\_glimpse: continuous", 
        linesep = c("", "", "\\addlinespace")) %>%
  kable_styling(latex_options = c("scale_down", "hold_position")) %>% 
  column_spec(1, width = "4cm")
sum_wcgs[[2]] %>% 
  select(-c(5, 9)) %>% 
  kable(row.names = FALSE, align = c("l", "l", "l", "r", "r", "r", "r", "r", "r", "r"), 
        booktabs = TRUE, caption = "WCGS data, ff\\_glimpse: categorical", 
        linesep = c("", "", "\\addlinespace")) %>%
  kable_styling(latex_options = c("scale_down", "hold_position")) %>%  
  column_spec(6, width = "3cm") %>% 
  column_spec(7, width = "3cm")
```

### Plot the data

```{r echo=FALSE, message=FALSE}
mykable = function(x, caption = "CAPTION", ...){
  kable(x, row.names = FALSE, align = c("l", "l", "r", "r", "r", "r", "r", "r", "r"), 
        booktabs = TRUE, caption = caption, 
        linesep = c("", "", "\\addlinespace"), ...) %>%
    kable_styling(latex_options = c("scale_down", "hold_position"))
}
```

```{r fig.height=3, fig.width=4.5, fig.cap="Scatter and line plot. Systolic blood pressure by weight and personality type."}
mydata %>%
  ggplot(aes(y = sbp, x = weight,
             colour = personality_2L)) +   # Personality type
  geom_point(alpha = 0.2) +                # Add transparency
  geom_smooth(method = "lm", se = FALSE)
```

From this plot we can see that there is a weak relationship between weight and blood pressure. 
In addition, there is really no meaningful effect of personality type on blood pressure. 
This is really important because, as you will see below, we are about to find some highly statistically significant effects in a model.

### Linear regression with finalfit
\index{linear regression@\textbf{linear regression}!finalfit}

**Finalfit** is our own package and provides a convenient set of functions for fitting regression models with results presented in final tables.

There are a host of features with example code at the [finalfit website](https://finalfit.org).

Here we will use the all-in-one `finalfit()` function, which takes a dependent variable and one or more explanatory variables. 
The appropriate regression for the dependent variable is performed, from a choice of linear, logistic, and Cox Proportional Hazards regression. 
Summary statistics, together with a univariable and a multivariable regression analysis are produced in a final results table. 

```{r message=FALSE}
dependent = "sbp"
explanatory = c("personality_2L")
fit_sbp1 = mydata %>% 
  finalfit(dependent, explanatory, metrics = TRUE)
```
\index{functions@\textbf{functions}!finalfit}

```{r, echo=FALSE}
fit_sbp1[[1]] %>% mykable(caption = "Linear regression: Systolic blood pressure by personality type.") %>% 
  column_spec(1, width = "4cm")
fit_sbp1[[2]] %>% mykable(caption = "Model metrics: Systolic blood pressure by personality type.", col.names = "") %>% 
  column_spec(1, width = "18cm")
```

Let's look first at our explanatory variable of interest, personality type.
When a factor is entered into a regression model, the default is to compare each level of the factor with a "reference level".
This reference level can be easily changed (see section \@ref(chap08-h2-fct-relevel).
Alternatives methods are available (sometimes called *contrasts*), but the default method is likely to be what you want almost all the time. 
Note this is sometimes referred to as creating a "dummy variable". 

It can be seen that the mean blood pressure for type A is higher than for type B. 
As there is only one variable, the univariable and multivariable analyses are the same (the multivariable column can be removed if desired by including `select(-5) #5th column` in the piped function). 

Although the difference is numerically quite small (2.3 mmHg), it is statistically significant partly because of the large number of patients in the study. 
The optional `metrics = TRUE` output gives us the number of rows (in this case subjects) included in the model. 
This is important as frequently people forget that in standard regression models, missing data from any variable results in the that entire row  being excluded from the analysis (see chapter \@ref(chap13-h1). 

Note the `AIC` and `Adjusted R-squared` results. 
The adjusted R-squared is very low - the model only explains only 0.6% of the variation in systolic blood pressure. 
This is to be expected, given our scatterplot above. 

Let's now include subject weight, which we have hypothesised may influence blood pressure. 

```{r message=FALSE}
dependent = "sbp"
explanatory = c("weight", "personality_2L")
fit_sbp2 = mydata %>% 
  finalfit(dependent, explanatory, metrics = TRUE)
```

```{r, echo=FALSE}
fit_sbp2[[1]] %>% mykable(caption = "Multivariable linear regression: Systolic blood pressure by personality type and weight.") %>% 
  column_spec(1, width = "4cm")
fit_sbp2[[2]] %>% mykable(caption = "Multivariable linear regression metrics: Systolic blood pressure by personality type and weight.", col.names = "") %>% 
  column_spec(1, width = "18cm")
```

The output shows us the range for weight (78 to 320 pounds) and the mean (standard deviation) systolic blood pressure for the whole cohort. 

The coefficient and 95% confidence interval are provided by default. 
This is interpreted as: for each pound increase in weight, there is on average a corresponding increase of 0.18 mmHg in systolic blood pressure.  

Note the difference in the interpretation of continuous and categorical variables in the regression model output (Figure \@ref(fig:chap07-fig-types)). 

The adjusted R-squared is now higher - the personality and weight together explain 6.8% of the variation in blood pressure.  
The AIC is also slightly lower meaning this new model better fits the data. 

There is little change in the size of the coefficients for each variable in the multivariable analysis, meaning that they are reasonably independent. 
As an exercise, check the the distribution of weight by personality type using a boxplot. 

Let's now add in other variables that may influence systolic blood pressure.


```{r message=FALSE}
dependent = "sbp"
explanatory = c("personality_2L", "weight", "age", 
                "height", "chol", "smoking") 
fit_sbp3 = mydata %>% 
  finalfit(dependent, explanatory, metrics = TRUE)
```

```{r, echo=FALSE}
fit_sbp3[[1]] %>% mykable(caption = "Multivariable linear regression: Systolic blood pressure by available explanatory variables.") %>% 
  column_spec(1, width = "4cm")
fit_sbp3[[2]] %>% mykable(caption = "Model metrics: Systolic blood pressure by available explanatory variables.", col.names = "") %>% 
  column_spec(1, width = "18cm")
```

Age, height, serum cholesterol, and smoking status have been added. 
Some of the variation explained by personality type has been taken up by these new variables - personality is now associated with an average change of blood pressure of 1.4 mmHg.

The adjusted R-squared now tells us that 12% of the variation in blood pressure is explained by the model, which is an improvement. 

Look out for variables that show large changes in effect size or a change in the direction of effect when going from a univariable to multivariable model. 
This means that the other variables in the model are having a large effect on this variable and the cause of this should be explored. 
For instance, in this example the effect of height changes size and direction. 
This is because of the close association between weight and height. 
For instance, it may be more sensible to work with body mass index ($weight / height^2$) rather than the two septate variables. 
This can be created easily. 

In general, variables that are highly correlated with each other should be treated carefully in regression analysis. 
This is called collinearity and can lead to unstable estimates of coefficients. 
For more on this, see section \@ref(chap09-h2-multicollinearity).

```{r}
mydata = mydata %>% 
  mutate(
    bmi = ((weight*0.4536) / (height*0.0254)^2) %>% 
      ff_label("BMI")
  )
```

Weight and height can be replaced in the model with BMI. 

```{r message=FALSE}
explanatory = c("personality_2L", "bmi", "age", 
                "chol", "smoking") 

fit_sbp4 = mydata %>% 
  finalfit(dependent, explanatory, metrics = TRUE)
```

```{r, echo=FALSE}
fit_sbp4[[1]] %>% mykable(caption = "Multivariable linear regression: Systolic blood pressure using BMI.") %>% 
  column_spec(1, width = "4cm")
fit_sbp4[[2]] %>% mykable(caption = "Model metrics: Systolic blood pressure using BMI.", col.names = "") %>% 
  column_spec(1, width = "18cm")
```

On the principle of parsimony, we may want to remove variables which are not contributing much to the model. 
For instance, let's compare models with and without the inclusion of smoking. 
This can be easily done using the `finalfit` `explanatory_multi` option.

```{r message=FALSE}
dependent = "sbp"
explanatory = c("personality_2L", "bmi", "age", 
                "chol", "smoking") 
explanatory_multi = c("bmi", "personality_2L", "age", 
                      "chol") 
fit_sbp5 = mydata %>% 
  finalfit(dependent, explanatory, 
           explanatory_multi, 
           keep_models = TRUE, metrics = TRUE)
```

```{r, echo=FALSE}
fit_sbp5[[1]] %>% mykable(caption = "Multivariable linear regression: Systolic blood pressure by available explanatory variables and reduced model.") %>% 
  column_spec(1, width = "4cm") %>% 
  column_spec(4, width = "4cm") %>% 
  column_spec(5, width = "4cm") %>% 
  column_spec(6, width = "4cm")
fit_sbp5[[2]] %>% mykable(caption = "Model metrics: Systolic blood pressure by available explanatory variables (top) with reduced model (bottom).", col.names = "") %>% 
  column_spec(1, width = "18cm")
```

This results in little change in the other coefficients and very little change in the AIC. 
We will consider the reduced model the final model. 

We can check the assumptions as above.

```{r fig.height=5, fig.width=5, message=FALSE, fig.cap="Diagnostic plots: linear regression model of systolic blood pressure."}
dependent = "sbp"
explanatory_multi = c("bmi", "personality_2L", "age", 
                      "chol") 
mydata %>%
  lmmulti(dependent, explanatory_multi) %>% 
  autoplot()
```


An important message in the results relates to the highly significant *p*-values in the table above. 
Should we conclude that in a "multivariable regression model controlling for BMI, age, and serum cholesterol, blood pressure was significantly elevated in those with a Type A personality (1.56 (0.57 to 2.56, p=0.002) compared with Type B?
The *p*-value looks impressive, but the actual difference in blood pressure is only 1.6 mmHg. 
Even at a population level, that seems unlikely to be clinically significant. 
Which fits with our first thoughts when we saw the scatter plot. 

This serves to emphasise our most important point.
Our focus should be on understanding the underlying data itself, rather than relying on complex multidimensional modelling procedures. 
By making liberal use of upfront plotting, together with further visualisation as you understand the data, you will likely be able to draw most of the important conclusions that the data has to offer. 
Use modelling to quantify and confirm this, rather than the primary method of data exploration. 

<!-- Coefficient plot -->
<!-- Consider lmer / random effects -->

### Summary

Time spent truly understanding linear regression is well spent. 
Not because you will spend a lot of time making linear regression models in health data science (we rarely do), but because it the essential foundation for understanding more advanced statistical models. 

It can even be argued that all [common statistical tests are linear models](https://lindeloev.github.io/tests-as-linear).
This great post demonstrates beautifully how the statistical tests we are most familiar with (such as t-test, Mann-Whitney U test, ANOVA, chi-squared test) can simply be considered as special cases of linear models, or a close approximations. 

Regression is fitting lines, preferably straight, through data points. 
Make $\hat{y} = \beta_0 + \beta_1 x_1$ a close friend.

<!-- ## Exercise 5 -->

<!-- 1. Copy the multivariable linear regression model (where "Turkey" and the "United Kingdom" are in the same `lm()` model) from Exercise 4. -->

<!-- 2. Include a third country (e.g. "Portugal") in the `filter(country %in% c("Turkey", "United Kingdom", "Portugal"))` to your multivariable linear regression model. -->

<!-- 3. Do the results change? How, and why? -->

<!-- ```{r} -->
<!-- # Exercise 5 - your R code -->
<!-- ``` -->


<!-- ## Exercise 2 -->

<!-- Open the first Shiny app ("Simple regression"). Move the sliders until the red lines (residuals*) turn green - this means you've made the line fit the points as well as possible. Look at the intercept and slope - discuss with your neighbour or a tutor what these numbers mean/how they affect the straight line on the plot. -->

<!-- *Residual is how far away each point (observation) is from the linear regression line. (In this example it's the linear regression line, but residuals are relevant in many other contexts as well.) -->



<!-- ## Solutions -->

<!-- ## Exercise 1 solution -->

<!-- ```{r} -->
<!-- mydata %>%  -->
<!--   filter(country %in% c("United Kingdom", "Turkey")) %>%  -->
<!--   ggplot(aes(x = year, y = lifeExp)) + -->
<!--   geom_point() + -->
<!--   facet_wrap(~country) + -->
<!--   theme_bw() + -->
<!--   scale_x_continuous(breaks = c(1960, 1980, 2000)) + -->
<!--   geom_smooth(method = "lm") -->
<!-- ``` -->

<!-- ## Exercise 5 solution -->


<!-- ```{r} -->

<!-- mydata %>%  -->
<!--   filter(country %in% c("Turkey", "United Kingdom", "Portugal")) %>%  -->
<!--   lm(lifeExp ~ year_from1952*country, data = .)   %>%  -->
<!--   tidy() %>% -->
<!--   mutate(estimate = round(estimate, 2)) %>%  -->
<!--   select(term, estimate) -->

<!-- ``` -->

<!-- Overall, the estimates for Turkey and the UK do not change, but Portugal becomes the reference (alphabetically first) and you need to subtract or add the relevant lines for Turkey and UK to get their intercept values. -->







<!-- ## Exercise 4 -->

<!-- Convince yourself that using an fully interactive multivariable model is the same as running several separate simple linear regression models. Remember that we calculate the life expectancy in 1952 (intercept) and improvement per year (slope) for Turkey and the United Kingdom: -->

<!-- ```{r} -->
<!-- fit_uk %>% -->
<!--   tidy() %>% -->
<!--   mutate(estimate = round(estimate, 2)) %>%  -->
<!--   select(term, estimate) -->
<!-- ``` -->


<!-- ```{r} -->
<!-- fit_turkey %>% -->
<!--   tidy() %>% -->
<!--   mutate(estimate = round(estimate, 2)) %>%  -->
<!--   select(term, estimate) -->
<!-- ``` -->

<!-- These were the two separate models from above (now using `tidy() %>%  mutate() %>%  select()` instead of `summary()`). And this is how we can get to the same coefficients using a single multivariable linear regression model (note the `year_from1952*country`): -->

<!-- ```{r} -->
<!-- mydata %>%  -->
<!--   filter(country %in% c("Turkey", "United Kingdom")) %>%  -->
<!--   lm(lifeExp ~ year_from1952 * country, data = .)   %>%  -->
<!--   tidy() %>% -->
<!--   mutate(estimate = round(estimate, 2)) %>%  -->
<!--   select(term, estimate) -->
<!-- ``` -->

<!-- Now. It may seem like R has omitted Turkey but the values for Turkey are actually in the Intercept = 46.02 and in year_from1952 = 0.50. Can you make out the intercept and slope for the UK? Are they the same as in the simple linear regression model? -->

