# Linear regression

## Regression

Regression is a method with which we can determine the existence and strength of the relationship between two or more variables. 
This can be thought of as drawing lines, ideally straight lines, through data points. 

Linear regression is our method of choice for examining continuous outcome variables. 
Broadly, there are often two separate goals in regression:

* Prediction: fitting a predictive model to an observed dataset. Using that model to make predictions about an outcome from a new set of explanatory variables;
* Explanation: fit a model to explain the inter-relationships between a set of variables.

Figure \@ref(fig:chap07-fig-regression) unifies the terms we will use throughout. 
A clear scientific question should define our `explanatory variable of interest` (*x*), which sometimes gets called a exposure, predictor, or independent variable. 
Our outcome of interest will be referred to as the `dependent` variable (*y*), sometimes referred to as the response. 
In simple linear regression, there is a single explanatory variable and a dependent variable, and we will sometimes refer to this as *univariable linear regression*. 
When there is more than one explanatory variable, we will call this *multivariable regression*. 
Avoid the term *multivariate regression*, which suggests more than one dependent variable. 
We don't use this method and we suggest you don't either!

Note that the dependent variable is always continuous, it cannot be a categorical variable. 
The explanatory variables can be either continuous or categorical. 

## The Question (1)

We will illustrate our examples of linear regression using a classical question which is important to many of us!
This is the relationship between coffee consumption and blood pressure (and therefore cardiovascular events, such as myocardial infarction and stroke).
There has been a lot of backwards and forwards over decades about whether coffee is harmful, has no effect, or is in fact beneficial. 
Figure \@ref(fig:chap07-fig-regression) shows a linear regression example. 
Each point is a person and average number of cups of coffee per day is the explanatory variable of interest (*x*) and systolic blood pressure as the dependent variable (*y*). 
This next bit is important!
These data are made up, fake, randomly generated, fabricated, not real. 
So please do not alter your coffee habit on the basis of these plots!

```{r chap07-fig-regression, echo = FALSE, fig.cap="The anatomy of a regression plot."}
knitr::include_graphics("images/chapter07/1_regression_terms.pdf")
```

## Fitting a regression line

Simple linear regression uses the *ordinary least squares* method for fitting. 
The details of this are beyond the scope here, but if you want to get out the linear algebra/matrix maths you did in high school, an enjoyable afternoon can be spent proving to yourself how it actually works. 

Figure \@ref(fig:chap07-fig-residuals) aims to make this easy to understand. 
The maths defines a line which best fits the data provided. 
For the line to fit best, the distances between it and the observed data should be as small as possible. 
The distance from each observed point to the line is called a *residual* - one of those statistical terms that bring on the sweats. 
It just refers to the "residual error" left over after the line is fitted. 

You can use the [simple regression shiny app](https://argoshare.is.ed.ac.uk/simple_regression) to explore the concept. 
We want the residuals to be as small as possible. 
We can square each residual (to get rid of minuses and penalise further away points) and add them up. 
If this number is as small as possible, the line is fitting as best it can. 
Or in more formal language, we want to minimise the sum of squared residuals. 

```{r chap07-fig-residuals, echo = FALSE, fig.cap="How a regression line is fitted."}
knitr::include_graphics("images/chapter07/2_residuals.pdf")
```

## When the line fits well

Linear regression modelling has four main assumptions:

1. Linear relationship;
2. Independence of residuals;
3. Normal distribution of residuals;
4. Equal variance of residuals. 

You can use the [simple regression diagnostics shiny app](https://argoshare.is.ed.ac.uk/simple_regression_diagnostics) to get a handle on these. 

Figure \@ref(fig:chap07-fig-diags) shows diagnostic plots from the app, which we will run ourselves below. 

### Linear relationship

A simple scatter plot should show a linear relationship between the explanatory and the dependent variable, as in figure \@ref(fig:chap07-fig-diags)A. 
If the data describe a non-linear pattern (figure \@ref(fig:chap07-fig-diags)B), then a straight line is not going to fit it very well. 
In this situation, an alternative model should be considered, such as including a quadratic (x^2^) or polynomial term. 

### Independence of residuals

The observations and therefore the residuals should be independent.
This is more commonly a problem in time series data, where observations may be correlated across time with each other (autocorrelation). 


### Normal distribution of residuals

The observations should be normally distributed around the fitted line. 
This means that the residuals should show a normal distribution with a mean of zero (figure \@ref(fig:chap07-fig-diags)A).
If the observations are not equally distributed around the line, the histogram of residuals will be skewed and a normal Q-Q plot will show residuals diverging from the 45 degree line (figure \@ref(fig:chap07-fig-diags)B). 
See *Q-Q plot ref*. 

### Equal variance of residuals

The distribution of the observations around the fitted line should be the same on the left side of the scatter plot as they are on the right side. 
Look at the fan-shaped data on the [simple regression diagnostics shiny app](https://argoshare.is.ed.ac.uk/simple_regression_diagnostics). 
This should be obvious on the residuals vs. fitted values plot, as well as the histogram and normal Q-Q plot. 

This is really all about making sure that the line you draw through your data points is valid.
It is about ensuring that the regression line is valid across the range of the explanatory variable and dependent variable.
It is really about understanding the underlying data, rather than relying on a fancy statistical test that gives you a p-value. 

```{r chap07-fig-diags, echo = FALSE, fig.cap="Regression diagnostics."}
knitr::include_graphics("images/chapter07/3_diags.pdf")
```

## The fitted line and the linear equation

We promised to keep the equations to a minimum, but this one is so important it needs to be included. But it is easy to understand, so fear not. 

Figure \@ref(fig:chap07-fig-equation) links the fitted line, the linear equation, and the output from R. Some of this will likely be already familiar to you. 

Figure \@ref(fig:chap07-fig-equation)A shows a scatter plot with fitted lines from a multivariable linear regression model. 
The plot is taken from the [multivariable regression shiny app](https://argoshare.is.ed.ac.uk/multi_regression/). 
Remember, these data are simulated and are not real. 
This app will really help you understand different regression models, more on this below. 
The app allows us to specify "the truth" with the sliders on the left hand side. 
For instance, we can set the intercept=1, meaning that when all $x=0$, the value of the dependent variable, $\hat{y}=1$.  

Our model has a continuous explanatory variable of interest (average coffee consumption) and a further categorical variable (smoking). 
In the example the truth is set as $intercept=1$, $\beta_1=1$ (true effect of coffee on blood pressure, gradient/slope of line), and $\beta_2=2$ (true effect of smoking on blood pressure).
The points on the plot are simulated following the addition of random noise. 

Figure \@ref(fig:chap07-fig-equation)B shows the default output in R for this linear regression model. 
Look carefully and make sure you are clear how the fitted lines, the linear equation, and the R output fit together. 
In this example, the random sample from our true population specified above shows $intercept=0.67$, $\beta_1=1.00$ (coffee), and $\beta_2=2.48$ (smoking). 
A *p*-value is provided ($Pr(> \left| t \right|)$), which is the result of a null hypothesis significance test for the gradient of the line being equal to zero. Said another way, this is the probability that the gradient of the particular line is equal to zero. 

```{r chap07-fig-equation, echo = FALSE, fig.cap="Linking the fitted line, regression equation and R output."}
knitr::include_graphics("images/chapter07/4_equation.pdf")
```

## Effect modification

Effect modification occurs when the size of the effect of the explanatory variable of interest (exposure) on the outcome (dependent variable) differs depending on the level of a third variable. 
Said another way, this is a situation in which an explanatory variable differentially (positively or negatively) modifies the observed effect of another explanatory variable on the outcome.

Again, this is best thought about using the concrete example provided in the [multivariable regression shiny app](https://argoshare.is.ed.ac.uk/multi_regression/).

Figure \@ref(fig:chap07-fig-dags) shows three potential causal pathways. 

In the first, smoking is not associated with the outcome (blood pressure) or our explanatory variable of interest (coffee consumption). 

In the second, smoking is associated with elevated blood pressure, but not with coffee consumption. 
This is an example of effect modification. 

In the third, smoking is associated with elevated blood pressure and with coffee consumption. 
This is an example of confounding. 

```{r chap07-fig-dags, echo = FALSE, fig.cap="Causal pathways, effect modification and confounding."}
knitr::include_graphics("images/chapter07/5_dags.pdf")
```

### Additive vs. multiplicative effect modification (interaction)

Depending on the field you work, will depend on which set of terms you use. 
Effect modification can be additive or multiplicative. 
We refer to multiplicative effect modification as simply including a statistical interaction. 

Figure \@ref(fig:chap07-fig-types) should make it clear exactly how these work. 
The data have been set-up to include an interaction term. 
What does this mean? 

* $intercept=1$: the blood pressure ($\hat{y}$) for non-smokers who drink no coffee (all $x=0$);
* $\beta_1=1$ (`coffee`): the additional blood pressure for each cup of coffee drunk by non-smokers (slope of the line when $x_2=0$;
* $\beta_2=1$ (`smoking`): the difference in blood pressure between non-smokers and smokers who drink no coffee ($x_1=0$);
* $\beta_3=1$ (`coffee:smoking` interaction): the blood pressure ($\hat{y}$) in addition to $\beta_1$ and $\beta_2$, for each cup of coffee drunk by smokers ($x_2=1)$.

You may have to read that a couple of times in combination with looking at Figure \@ref(fig:chap07-fig-types).

With the additive model, the fitted lines for non-smoking vs smoking are constrained to be parallel. 
Look at the equation in Figure \@ref(fig:chap07-fig-types)B and convince yourself that the lines can never be anything other than parallel. 

A statistical interaction (or multiplicative effect modification) is a situation where the effect of an explanatory variable on the outcome is modified in non-additive manner. 
In other words using our example, the fitted lines are no longer constrained to be parallel.

If we had not checked for an interaction effect, we would have inadequately described the true relationship between these three variables. 

What does this mean back in reality? 
Well it may be biologically plausible for the effect of smoking on blood pressure to increase multiplicatively due to a a chemical interaction between cigarette smoke and caffeine, for example.

Note, we are just trying to find a model which best describes the underlying data. 
All models are approximations of reality. 

```{r chap07-fig-types, echo = FALSE, fig.cap="Multivariable linear regression with additive and multiplicative effect modification."}
knitr::include_graphics("images/chapter07/6_types.pdf")
```

## R-squared and model fit

Figure \@ref(fig:chap07-fig-types) includes a further metric from the R output: `Adjusted R-squared`. 

R-squared is another measure of how close the data are to the fitted line. 
It is also known as the coefficient of determination and represents proportion of the dependent variable which is explained by the explanatory variable(s). 
So 0.0 indicates that none of the variability in the dependent is explained by the explanatory (no relationship between data points and fitted line) and 1.0 indicates that the model explains all of the variability in the dependent (fitted line follows data points exactly).

R provides the `R-squared` and the `Adjusted R-squared`. 
The adjusted R-squared includes a penalty the more explanatory variables are included in the model. 
So if the model includes variables which do not contribute to the description of the dependent variable, the adjusted R-squared will be lower. 

Looking again at Figure \@ref(fig:chap07-fig-types), in A, a simple model of coffee alone does not describe the data well (adjusted R-squared 0.38). 
Add smoking to the model improves the fit as can be seen by the fitted lines (0.87). But a true interaction exists in the actual data. 
By including this interaction in the model, the fit is very good indeed (0.93).

## Confounding

The last important concept to mention here is confounding. 
Confounding is a situation in which the association between an explanatory variable (exposure) and outcome (dependent variable) is distorted by the presence of another explanatory variable.

In our example, confounding exists if there is an association between smoking and blood pressure AND smoking and coffee consumption (Figure \@ref(fig:chap07-fig-dags)C). 
This exists simply if smokers drink more coffee than non-smokers. 

Figure \@ref(fig:chap07-fig-confounding) shows this really clearly. 
The underlying data have been altered so that those who drink more than two cups of coffee per day also smoke and those who drink fewer than two cups per day do not smoke. 
A true effect of smoking on blood pressure is entered, but NO effect of coffee on blood pressure. 

If we simply fit blood pressure by coffee consumption (Figure \@ref(fig:chap07-fig-confounding)A), then we may mistakenly conclude a relationship between coffee consumption and blood pressure. 
But this does not exist, because the ground truth we have set is that no relationship exists between coffee and blood pressure. 
We are simply seeing the effect of smoking on blood pressure, which is confounding the effect of coffee on blood pressure. 

If we include the confounder in the model by adding smoking, the true relationship becomes apparent. 
Two parallel flat lines indicating no effect of coffee on blood pressure, but a relationship between smoking and blood pressure. 
This procedure is often referred to as controlling for or adjusting for confounders. 

```{r chap07-fig-confounding, echo = FALSE, fig.cap="Multivariable linear regression with confounding of coffee drinking by smoking."}
knitr::include_graphics("images/chapter07/7_confounding.pdf")
```

## Summary

We have intentionally spent some time going through the principles and applications of linear regression because it is so important. 
A firm grasp of these concepts lead to an easy understanding of other regression procedures, such as logistic regression and Cox Proportional Hazards regression. 

We will now perform all this ourselves in R using a gapminder dataset which you are familiar with from preceding chapters. 

## Fitting simple models

### The Question (2)
We are interested in modelling the change in life expectancy for different countries over the past 60 years.  

### Get the data
```{r, message=F}
library(tidyverse)
library(gapminder) # dataset
library(lubridate) # handles dates
library(finalfit)
library(broom)

theme_set(theme_bw())
mydata = gapminder
```

### Check the data
Always check a new dataset, as described in 06-1 
<!-- need ref -->

```{r eval=FALSE}
glimpse(mydata) # each variable as line, variable type, first values
missing_glimpse(mydata) # missing data for each variable
ff_glimpse(mydata) # summary statistics for each variable
```

### Plot the data

Let's plot the life expectancies in European countries over the past 60 years, focussing on the UK and Turkey. 

```{r, fig.height=7, fig.width=7, fig.cap="Scatterplot: Life expectancy by year in European countries"}
p = mydata %>%                         # save as object p
  filter(continent == "Europe") %>%    # Europe only
  ggplot(aes(x = year, y = lifeExp)) + # lifeExp~year  
  geom_point() +                       # plot points
  facet_wrap(~ country) +              # facet by country
  scale_x_continuous(
    breaks = c(1960, 1980, 2000))      # adjust x-axis   
p                                      # view result
```

We can add in simple best fit lines using `ggplot` directly

```{r, fig.height=7, fig.width=7, fig.cap="Scatter and line plot: Life expectancy by year in European countries with best fit simple linear regression line."}
p +
  geom_smooth(method = "lm")
```

### Simple linear regression

As you can see, `ggplot()` is very happy to run and plot linear regression models for us.
While this is sometimes convenient, we usually want to build, run, and explore these models ourselves. 
We can then investigate the intercepts and the slope coefficients (linear increase per year):

First let's plot two countries to compare, Turkey and United Kingdom

```{r}
mydata %>% 
  filter(country %in% c("Turkey", "United Kingdom")) %>% 
  ggplot(aes(x = year, y = lifeExp, colour = country)) + 
  geom_point()
```

The two non-parallel lines may make you think of what has been discussed above. 

First, let's model the two countries separately. 

United Kingdom:
```{r}
fit_uk = mydata %>%
  filter(country == "United Kingdom") %>% 
  lm(lifeExp~year, data = .)

fit_uk %>% 
  summary()
```

Turkey: 
```{r}
fit_turkey = mydata %>%
  filter(country == "Turkey") %>% 
  lm(lifeExp~year, data = .)

fit_turkey %>% 
  summary()
```

*Probably covered enough already ->*

### When pipe sends data to the wrong place: use `, data = .` to direct it

In the code above, the `, data = .` bit is necessary because the pipe usually sends data to the beginning of function brackets. So `mydata %>% lm(lifeExp~year)` would be equivalent to `lm(mydata, lifeExp~year)`. However, this is not an order that `lm()` will accept. `lm()` wants us to specify the variables first (`dependent~explanatory`), and then wants the data these variables are present in. So we have to use the `.` to tell the pipe to send the data to the second argument of `lm()`, not the first.

#### Accessing the coefficients of linear regression

A simple linear regression model will return two coefficients - the intercept and the slope (the second returned value). 
Compare this to the `summary()` output above.

```{r}
fit_uk$coefficients
```

```{r}
fit_turkey$coefficients
```

In this example, the intercept is telling us that life expectancy at year 0 in the United Kingdom (some 2000 years ago) was -294 years. While this is mathematically correct (based on the data we have), it makes no sense in practice. It is important at all stages of data analysis, to keep "sense checking" your results.

To make the intercepts meaningful, we will add in a new column called `year_from1952` and re-run `fit_uk` and `fit_turkey` using `year_from1952` instead of `year`.

```{r}
mydata = mydata %>% 
  mutate(year_from1952 = year - 1952)

fit_uk = mydata %>%
  filter(country == "United Kingdom") %>% 
  lm(lifeExp ~ year_from1952, data = .)

fit_turkey = mydata %>%
  filter(country == "Turkey") %>% 
  lm(lifeExp ~ year_from1952, data = .)
```


```{r}
fit_uk$coefficients
```


```{r}
fit_turkey$coefficients
```

Now, the updated results tell us that in year 1952, the life expectancy in the United Kingdom was 68 years. 
Note that the slope (0.18) does not change. There was nothing wrong with the original model and the results were correct, the intercept was just not very useful.

#### Accesing all model information `tidy()` and `glance()`

In the fit_uk and fit_turkey examples above, we were using `fit_uk %>% summary()` to get R to print out a summary of the model. This summary is not, however, in a rectangular shape so we can't easily access the values or put them in a table/use as information on plot labels.

We use the `tidy()` function from `library(broom)` to get the explanatory variable specific values in a nice tibble:

```{r}
fit_uk %>% tidy()
```

In the `tidy()` output, the column `estimate` includes both the intercepts and slopes.

And we use the `glance()` function to get overall model statistics (mostly the r.squared).
```{r}
fit_uk %>% glance()
```

### Multivariable linear regression

Multivariable linear regression includes more than one explanatory variable. There are a few ways to include more variables, depending on whether they should share the intercept and how they interact:

Simple linear regression (exactly one predictor variable):

`myfit = lm(lifeExp ~ year, data = mydata)`

Multivariable linear regression (additive):

`myfit = lm(lifeExp ~ year + country, data = mydata)`

Multivariable linear regression (interaction):

`myfit = lm(lifeExp ~ year * country, data = mydata)`

This equivalent to:
`myfit = lm(lifeExp ~ year + country + year:country, data = mydata)`

These examples of multivariable regression include two variables: `year` and `country`, but we could include more by adding them with `+`.

In this particular setting, it will become obvious which model is appropriate. So we have complete control over the model being fitted, we will use the `predict()` function directly to obtain our fitted line, rather than leaving it up to `ggplot`.  

#### Model 1: year only

```{r}
mydata_UK_T = mydata %>% 
  filter(country %in% c("Turkey", "United Kingdom"))

fit_both1 = mydata_UK_T %>% 
  lm(lifeExp ~ year_from1952, data = .)
fit_both1

pred_both1 = predict(fit_both1)

mydata_UK_T %>% 
  bind_cols(pred_both = pred_both1) %>% 
  ggplot() + 
  geom_point(aes(x = year, y = lifeExp, colour = country)) +
  geom_line(aes(x = year, y = pred_both))
```

By fitting year only, the model ignores country. This gives us a fitted line which is the average of life expectancy in the UK and Turkey. This may be desirable, depending on the question. But here we want to best describe the data. 

#### Model 2: year + country
```{r}
fit_both2 = mydata_UK_T %>% 
  lm(lifeExp ~ year_from1952 + country, data = .)
fit_both2

pred_both2 = predict(fit_both2)

mydata_UK_T %>% 
  bind_cols(pred_both = pred_both2) %>% 
  ggplot() + 
  geom_point(aes(x = year, y = lifeExp, colour = country)) +
  geom_line(aes(x = year, y = pred_both, colour = country))
```

This is better, by including country in the model, we now have fitted lines better represent the data. However, the lines are constrained to be parallel. This is discussed in detail above. We need to include an interaction term to allow the effect of year on life expectancy to vary by country in a non-additive manner. 

#### Model 3: year * country

```{r}
fit_both3 = mydata_UK_T %>% 
  lm(lifeExp ~ year_from1952 * country, data = .)
fit_both3

pred_both3 = predict(fit_both3)

mydata_UK_T %>% 
  bind_cols(pred_both = pred_both3) %>% 
  ggplot() + 
  geom_point(aes(x = year, y = lifeExp, colour = country)) +
  geom_line(aes(x = year, y = pred_both, colour = country))
```

This fits the data much better than the previous two models. You can check the R-squared using `summary(fit_both1)

*Pro tip*

```{r}
library(purrr)
list(fit_both1, fit_both2, fit_both3) %>% 
  map_df(glance, .id = "fit")

```

### Check assumptions

The assumptions of linear regression can be checked with diagnostic plots, either by passing the fitted object (`lm()` output) to base R `plot()`, or by using the more convenient function below. 

```{r}
library(ggfortify)
autoplot(fit_both3)
```

There is no clear problem with the residuals, as we would expect from the scatterplot with fitted lines. 

## Fitting more complex models

### The Question (3)

Finally in this chapter, we are going to fit a more complex linear regression model. Here, we will discuss variable selection and introduce the Akaike Information Criterion (AIC). 

We will introduce a new dataset: The Western Collaborative Grou Study. This classic includes data from 3154 healthy young men aged 39-59 from the San Francisco area who were assessed for their personality type. It aimed to capture the occrunce of coronary heart disease over the following 8.5 years. 

We will use it however to explore the relationship between systolic blood pressure (`sbp`), weight (`weight`) and personality type (`personality_2L`). 

Personality type is A: aggressive and B: passive. 

### AIC

The Akaike Information Criterion (AIC) is an alternative goodness-of-fit measure. In that sense, it is similar to the R-squared, but has a different statistical basis. It is useful because it can be used to help guide the best fit in generalised linear models such as logistic regression (ref: chapter 9). It is based on the likelihood but is also penalised for the number of variables present in the model. We aim to have as small an AIC as possible. 

### Model fitting principles

Our 4/5 principles here. 

### Get the data

```{r}
mydata = finalfit::wcgs #F1 here for details
```

### Check the data
```{r eval=FALSE}
library(dplyr)
library(finalfit)
glimpse(mydata) # each variable as line, variable type, first values
missing_glimpse(mydata) # missing data for each variable
ff_glimpse(mydata) # summary statistics for each variable
```

### Plot the data

```{r, echo = FALSE}
mykable = function(x, ...){
  knitr::kable(x, row.names = FALSE, align = c("l", "l", "r", "r", "r", "r", "r", "r", "r"), 
               booktabs = TRUE, caption = "CAPTION", ...) %>% 
    kableExtra::kable_styling(font_size = 6)
}
```

```{r}
mydata %>% 
  ggplot(aes(y = sbp, x = weight, 
             colour = personality_2L)) +   # Personality type
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```

From this plot we can see that there is a weak relationship between weight and blood pressure. In addition, there is really no meaningful effect of personality type on blood pressure. This is important, because as you will see below, we are about to find statistically significant effects in a model. 

### Linear regression with Finalfit

Finalfit is our own package and provides a convenient set of functions for fitting regression models with results presented in final tables.

```{r}
dependent = "sbp"
explanatory = "weight"
fit_sbp1 = mydata %>% 
  finalfit(dependent, explanatory, metrics = TRUE)
```

```{r, echo=FALSE}
fit_sbp1[[1]] %>% mykable()
fit_sbp1[[2]] %>% mykable(col.names = "")
```

```{r}
dependent = "sbp"
explanatory = c("weight", "personality_2L")
fit_sbp2 = mydata %>% 
  finalfit(dependent, explanatory, metrics = TRUE)
```

```{r, echo=FALSE}
fit_sbp2[[1]] %>% mykable()
fit_sbp2[[2]] %>% mykable(col.names = "")
```


```{r}
dependent = "sbp"
explanatory = c("weight", "personality_2L", "age", "height", "chol", "smoking") 
fit_sbp3 = mydata %>% 
  finalfit(dependent, explanatory, metrics = TRUE)
```

```{r, echo=FALSE}
fit_sbp3[[1]] %>% mykable()
fit_sbp3[[2]] %>% mykable(col.names = "")
```


```{r}
dependent = "sbp"
explanatory = c("weight", "personality_2L", "age", "height", "chol", "smoking") 
explanatory_multi = c("weight", "personality_2L", "age", "height", "chol") 
fit_sbp4 = mydata %>% 
  finalfit(dependent, explanatory, 
           explanatory_multi, 
           keep_models = TRUE, metrics = TRUE)
```

```{r, echo=FALSE}
fit_sbp4[[1]] %>% mykable()
fit_sbp4[[2]] %>% mykable(col.names = "")
```

More to add for each step above.

An important message here relates to the highly significant p-values in the table above. 
Should we conclude that in a "multivariable regression model controlling for weight, height, age, serum cholesterol levels and smoking status, blood pressure was significantly elevated in those with a Type A personality (1.44 (95% CI 0.44 to 2.43, p=0.005) compared with Type B. 
The p-value looks impressive, but the actual difference in blood pressure is only 1.4 mmHg. 
Even at a population level, that seems unlikely to be clinically significant. 
Which fits with our first thoughts when we saw the scatter plot. 


<!-- ## Exercise 5 -->

<!-- 1. Copy the multivariable linear regression model (where "Turkey" and the "United Kingdom" are in the same `lm()` model) from Exercise 4. -->

<!-- 2. Include a third country (e.g. "Portugal") in the `filter(country %in% c("Turkey", "United Kingdom", "Portugal"))` to your multivariable linear regression model. -->

<!-- 3. Do the results change? How, and why? -->

<!-- ```{r} -->
<!-- # Exercise 5 - your R code -->
<!-- ``` -->


<!-- ## Exercise 2 -->

<!-- Open the first Shiny app ("Simple regression"). Move the sliders until the red lines (residuals*) turn green - this means you've made the line fit the points as well as possible. Look at the intercept and slope - discuss with your neighbour or a tutor what these numbers mean/how they affect the straight line on the plot. -->

<!-- *Residual is how far away each point (observation) is from the linear regression line. (In this example it's the linear regression line, but residuals are relevant in many other contexts as well.) -->



<!-- ## Solutions -->

<!-- ## Exercise 1 solution -->

<!-- ```{r} -->
<!-- mydata %>%  -->
<!--   filter(country %in% c("United Kingdom", "Turkey")) %>%  -->
<!--   ggplot(aes(x = year, y = lifeExp)) + -->
<!--   geom_point() + -->
<!--   facet_wrap(~country) + -->
<!--   theme_bw() + -->
<!--   scale_x_continuous(breaks = c(1960, 1980, 2000)) + -->
<!--   geom_smooth(method = "lm") -->
<!-- ``` -->

<!-- ## Exercise 5 solution -->


<!-- ```{r} -->

<!-- mydata %>%  -->
<!--   filter(country %in% c("Turkey", "United Kingdom", "Portugal")) %>%  -->
<!--   lm(lifeExp ~ year_from1952*country, data = .)   %>%  -->
<!--   tidy() %>% -->
<!--   mutate(estimate = round(estimate, 2)) %>%  -->
<!--   select(term, estimate) -->

<!-- ``` -->

<!-- Overall, the estimates for Turkey and the UK do not change, but Portugal becomes the reference (alphabetically first) and you need to subtract or add the relevant lines for Turkey and UK to get their intercept values. -->







<!-- ## Exercise 4 -->

<!-- Convince yourself that using an fully interactive multivariable model is the same as running several separate simple linear regression models. Remember that we calculate the life expectancy in 1952 (intercept) and improvement per year (slope) for Turkey and the United Kingdom: -->

<!-- ```{r} -->
<!-- fit_uk %>% -->
<!--   tidy() %>% -->
<!--   mutate(estimate = round(estimate, 2)) %>%  -->
<!--   select(term, estimate) -->
<!-- ``` -->


<!-- ```{r} -->
<!-- fit_turkey %>% -->
<!--   tidy() %>% -->
<!--   mutate(estimate = round(estimate, 2)) %>%  -->
<!--   select(term, estimate) -->
<!-- ``` -->

<!-- These were the two separate models from above (now using `tidy() %>%  mutate() %>%  select()` instead of `summary()`). And this is how we can get to the same coefficients using a single multivariable linear regression model (note the `year_from1952*country`): -->

<!-- ```{r} -->
<!-- mydata %>%  -->
<!--   filter(country %in% c("Turkey", "United Kingdom")) %>%  -->
<!--   lm(lifeExp ~ year_from1952 * country, data = .)   %>%  -->
<!--   tidy() %>% -->
<!--   mutate(estimate = round(estimate, 2)) %>%  -->
<!--   select(term, estimate) -->
<!-- ``` -->

<!-- Now. It may seem like R has omitted Turkey but the values for Turkey are actually in the Intercept = 46.02 and in year_from1952 = 0.50. Can you make out the intercept and slope for the UK? Are they the same as in the simple linear regression model? -->

