# Linear regression

## Regression

Regression is a method with which we can determine the existence and strength of the relationship between two or more variables. 
This can be thought of as drawing lines, ideally straight lines, through data points. 

Linear regression is our method of choice for examining continuous outcome variables. 
Broadly, there are often two separate goals in regression:

* Prediction: fitting a predictive model to an observed dataset. Using that model to make predictions about an outcome from a new set of explanatory variables;
* Explanation: fit a model to explain the inter-relationships between a set of variables.

Figure \@ref(fig:chap07-fig-regression) unifies the terms we will use throughout. 
A clear scientific question should define our `explanatory variable of interest` (*x*), which sometimes gets called a predictor. 
Our outcome of interest will be referred to as the `dependent` variable (*y*). 
In simple linear regression, there is a single explanatory variable and dependent variable, and we will call this *univariable linear regression*. 
When there is more than one explanatory variable, we will call this *multivariable regression*. Avoid the term *multivariate regression*, which suggests more than one dependent variable. We don't use this method and we suggest you don't either!

Note that the dependent variable is always continuous, it cannot be a categorical variable. 
The explanatory variables can be either continuous or categorical. 

## The Question

We will illustrate our examples of linear regression using a classical question which is important to many of us!
This is the relationship between coffee drinking and blood pressure (and therefore cardiovascular events, such as myocardial infarction and stroke).
There has been a lot of backwards and forwards over decades about whether coffee drinking is harmful, has no effect, or is in fact beneficial. 
Figure \@ref(fig:chap07-fig-regression) shows a linear regression example. 
Each point is a person and cups of coffee per day is the explanatory variable of interest (*x*) and systolic blood pressure as the dependent variable (*y*). 
This next bit is important!
These data are made up, fake, randomly generated, fabricated, not real. 
So please do not alter your coffee habit on the basis of these plots!

```{r chap07-fig-regression, echo = FALSE, fig.cap="The anatomy of a regression plot"}
knitr::include_graphics("images/chapter07/1_regression_terms.pdf")
```

## Fitting a regression line

Simple linear regression uses the *ordinary least squares* method for fitting. 
The details of this are beyond the scope here, but if you want to get out the linear algebra/matrix maths you did in high school, an enjoyable afternoon can be spent proving to yourself how it actually works. 

Figure \@ref(fig:chap07-fig-residuals) aims to make this easy to understand. 
The maths defines a line which best fits the data provided. 
For the line to fit best, the distances between it and the observed data should be as small as possible. 
The distance from each observed point to the line is called a *residual* - one of those statistical terms that bring on the sweats. 
It just refers to the "residual error" left over after the line is fitted. 

You can use the [simple regression shiny app](https://argoshare.is.ed.ac.uk/simple_regression) to explore the concept. 
We want the residuals to be as small as possible. 
We can square each residual (to get rid of minuses and penalise further away points) and add them up. 
If this number is as small as possible, the line is fitting as best it can. 
Or in more formal language, we want to minimise the sum of squared residuals. 

```{r chap07-fig-residuals, echo = FALSE, fig.cap="How a regression line is fitted"}
knitr::include_graphics("images/chapter07/2_residuals.pdf")
```

## When the line fits well

Linear regression modelling has four main assumptions:

1. Linear relationship;
2. Independence of residuals;
3. Normal distribution of residuals;
4. Equal variance of residuals. 

You can use the [simple regression diagnostics shiny app](https://argoshare.is.ed.ac.uk/simple_regression_diagnostics) to get a handle on these. 

Figure \@ref(fig:chap07-fig-diags) shows diagnostic plots from the app, which we will run ourselves below. 

### Linear relationship

A simple scatter plot should show a linear relationship between the explanatory and the dependent variable, as in figure \@ref(fig:chap07-fig-diags)A. 
If the data describe a non-linear pattern (figure \@ref(fig:chap07-fig-diags)B), then a straight line is not going to fit it very well. 
In this situation, an alternative model should be considered, such as including a quadratic (x^2^) or polynomial term. 

### Independence of residuals

The observations and therefore the residuals should be independent.
This is more commonly a problem in time series data, where observations may be correlated across time with each other (autocorrelation). 


### Normal distribution of residuals

The observations should be normally distributed around the fitted line. 
This means that the residuals should show a normal distribution with a mean of zero (figure \@ref(fig:chap07-fig-diags)A).
If the observations are not equally distributed around the line, the histogram of residuals will be skewed and a normal Q-Q plot will show residuals diverging from the 45 degree line (figure \@ref(fig:chap07-fig-diags)B). 
See *Q-Q plot ref*. 

### Equal variance of residuals

The distribution of the observations around the fitted line should be the same on the left side of the scatter plot as they are on the right side. 
Look at the fan-shaped data on the [simple regression diagnostics shiny app](https://argoshare.is.ed.ac.uk/simple_regression_diagnostics). 
This should be obvious on the residuals vs. fitted values plot, as well as the histogram and normal Q-Q plot. 


This is really all about making sure that the line you draw through your data points is valid.
It is about ensuring that the regression line is valid across the range of the explanatory variable and dependent variable.
It is really about understanding the underlying data, rather than relying on a fancy statistical test that gives you a p-value. 

```{r chap07-fig-diags, echo = FALSE, fig.cap="Regression diagnostics"}
knitr::include_graphics("images/chapter07/3_diags.pdf")
```

## Univariable vs multivariable regression

## Effect modification
Additive

## Interactions
of multiplicative effect modification. 

## Get the data
## Check the data
## Plot the data


## Data

We will be using the same gapminder dataset as in the last two sessions.

```{r, message=F}

library(tidyverse)
library(gapminder) # dataset
library(lubridate) # handles dates
library(broom)     # transforms statistical output to data frame

mydata = gapminder


```

## Plotting

Let's plot the life expectancies in European countries over the past 60 years:


```{r, fig.height=7, fig.width=7}

mydata %>% 
  filter(continent == "Europe") %>% 
  ggplot(aes(x = year, y = lifeExp)) +
  geom_point() +
  facet_wrap(~country) +
  theme_bw() +
  scale_x_continuous(breaks = c(1960, 1980, 2000))

```

### Exercise

Save the above filter into a new variable called `eurodata`:

```{r}

eurodata = mydata %>% 
  filter(continent == "Europe")

```



### Exercise

Create the same plot as above (life expectancy over time), but for just Turkey and the United Kingdom, and add linear regression lines. 
Hint: use `+ geom_smooth(method = "lm")` for the lines. `lm()` stands for linear model.

```{r, fig.width=5, fig.height=3, echo = FALSE}

mydata %>% 
  filter(country %in% c("United Kingdom", "Turkey") ) %>% 
  ggplot(aes(x = year, y = lifeExp)) +
  geom_point() +
  facet_wrap(~country) +
  theme_bw() +
  geom_smooth(method = "lm")


```

## Simple linear regression

As you can see, `ggplot()` is very happy to run and plot linear regression for us. 
To access the results, however, we should save the full results of the linear regression models into variables in our Environment. 
We can then investigate the intercepts and the slope coefficients (linear increase per year):

```{r, results = 'hold'}


fit_uk = mydata %>%
  filter(country == "United Kingdom") %>% 
  lm(lifeExp~year, data = .)  # the data=. argument is necessary


fit_turkey = mydata %>%
  filter(country == "Turkey") %>% 
  lm(lifeExp~year, data = .)


fit_uk$coefficients

fit_turkey$coefficients


```


### Exercise

To make the intercepts more meaningful, add a new column called `year_from1952` and redo `fit_turkey` and `fit_uk` using `year_from1952` instead of `year`.

```{r, results = 'hold'}

mydata$year_from1952 = mydata$year - 1952

fit_uk = mydata %>%
  filter(country == "United Kingdom") %>% 
  lm(lifeExp~year_from1952, data = .)


fit_turkey = mydata %>%
  filter(country == "Turkey") %>% 
  lm(lifeExp~year_from1952, data = .)


fit_uk$coefficients

fit_turkey$coefficients


```


### Model information: `summary()`, `tidy()` ,`glance()`

Accessing all other information about our regression model:

```{r}

fit_uk %>% summary()

fit_uk %>% tidy()

fit_uk %>% glance()

```


## If you are new to linear regression

See these interactive Shiny apps provided by RStudio:

https://gallery.shinyapps.io/simple_regression/

https://gallery.shinyapps.io/multi_regression/

(`library(shiny)` is an R package for making your output interactive)

### Exercise - Residuals

Open the first Shiny app ("Simple regression"). 
Move the sliders until the red lines (residuals*) turn green - this means you've made the line fit the points as well as possible. 
Look at the intercept and slope - discuss with your neighbour or a tutor what these numbers mean and how they affect the straight line on the plot.

*Residual is how far away each point (observation) is from the linear regression line. 
(In this example it's the linear regression line, but residuals are relevant in many other contexts as well.)

## Multiple linear regression

Multiple linear regression includes more than one predictor variable. 
There are a few ways to include more variables, depending on whether they should share the intercept and how they interact:

Simple linear regression (exactly one predictor variable):

`myfit = lm(lifeExp~year, data=eurodata)`

Multiple linear regression (additive):

`myfit = lm(lifeExp~year+country, data=eurodata)`

Multiple linear regression (all interactions):

`myfit = lm(lifeExp~year*country, data=eurodata)`


These examples of multiple regression include two variables: `year` and `country`, but we could include more by just adding them with `+`.

### Exercise

Open the second Shiny app ("Multiple regression") and see how:

* In simple regression, there is only one intercept and slope for the whole dataset.
* Using the additive model (`lm(formula = y ~ x + group`) the two lines (one for each group) have different intercepts but the same slope. However, the `lm()` summary seems to only include one line called "(Intercept)", how to find the intercept for the second group of points?
* Using the interactive model (`lm(formula = y ~ x*group`)) the two lines have different intercepts and different slopes.

### Exercise

Convince yourself that using an fully interactive multivariable model is similar to running several separate simple linear regression models. 
Remember that we calculate the life expectancy in 1952 (intercept) and improvement per year (slope) for Turkey and the United Kingdom:

```{r}
fit_uk %>%
  tidy() %>%
  mutate(estimate = round(estimate, 2)) %>% 
  select(term, estimate)

fit_turkey %>%
  tidy() %>%
  mutate(estimate = round(estimate, 2)) %>% 
  select(term, estimate)

```

(The lines `tidy()`, `mutate()`, and `select()` are only included for neater presentation here, you can use `summary()` instead.)

We can do this together using `year_from1952*country` in the `lm()`:

```{r}
mydata %>% 
  filter(country %in% c("Turkey", "United Kingdom")) %>% 
  lm(lifeExp ~ year_from1952*country, data = .)   %>% 
  tidy() %>%
  mutate(estimate = round(estimate, 2)) %>% 
  select(term, estimate)

```

Now. It may seem like R has omitted Turkey but the values for Turkey are actually in the Intercept = 46.02 and in year_from1952 = 0.50. 
Can you make out the intercept and slope for the UK? Are they the same as in the simple linear regression model?

### Exercise

Add a third country (e.g. "Portugal") to `filter(country %in% c("Turkey", "United Kingdom"))` in the above example. Do the results change?


### Optional (Advanced) Exercise

<!-- Replace do() with purrr map() on nested tibbles? Actually, the purrr example is below, maybe just remove the do() then? -->

Run separate linear regression models for every country in the dataset at the same time and putting it all in two neat data frames (one for the coefficients, one for the summary statistics):

```{r}


linfit_coefficients = mydata %>% 
  group_by(country) %>% 
  do(
    tidy(
      lm(lifeExp~year, data=.)
    )
  )


linfit_overall = mydata %>% 
  group_by(country) %>% 
  do(
    glance(
      lm(lifeExp~year, data=.)
    )
  )


```


Plot the linear regression estimate (improvement per year between 1952 -- 2007), size the points by their r-squared values, and colour the points by continent (hint: you will have to join `mydata`, `linfit_coefficients %>% filter(term == "year")`, and `linfit_overall`):

```{r, fig.width=4.5, fig.height=3}

mydata %>% 
  filter(year == 1952) %>% 
  full_join(linfit_coefficients %>% filter(term == "year"), by = "country") %>% 
  full_join(linfit_overall, by = "country") %>% 
  ggplot(aes(x = lifeExp, y = estimate, colour = continent, size = r.squared)) +
  geom_point(alpha = 0.6) +
  theme_bw() +
  scale_colour_brewer(palette = "Set1") +
  ylab("Increase in life expectancy per year") +
  xlab("Life expectancy in 1952")


```


## Very advanced example

Or you can do the above in a nested tibble/data frame:

```{r}
nested_linreg = mydata %>% 
  group_by(country) %>% 
  nest() %>% 
  mutate(model = purrr::map(data, ~ lm(lifeExp ~ year, data = .)))

```



## Solutions

**6.2.2**

```{r, fig.width=5, fig.height=3, eval = FALSE}

mydata %>% 
  filter(country %in% c("United Kingdom", "Turkey") ) %>% 
  ggplot(aes(x = year.formatted, y = lifeExp)) +
  geom_point() +
  facet_wrap(~country) +
  theme_bw() +
  geom_smooth(method = "lm")


```


**6.5.3**


```{r, eval = FALSE}

mydata %>% 
  filter(country %in% c("Turkey", "United Kingdom", "Portugal")) %>% 
  lm(lifeExp ~ year_from1952*country, data = .)   %>% 
  tidy() %>%
  mutate(estimate = round(estimate, 2)) %>% 
  select(term, estimate)

```

Overall, the estimates for Turkey and the UK do not change, but Portugal becomes the reference (alphabetically first) to which you can subtract or add the relevant lines for Turkey and the UK.




